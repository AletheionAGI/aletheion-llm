# Model Architecture
model:
  vocab_size: 50257
  n_layers: 6
  n_heads: 8
  d_model: 512
  d_ff: 2048
  max_seq_len: 512
  dropout: 0.1
  use_flash_attention: false
  tie_weights: true

# Training
training:
  batch_size: 32
  gradient_accumulation_steps: 1
  learning_rate: 3.0e-4
  weight_decay: 0.1
  max_steps: 100000
  warmup_steps: 2000
  lr_schedule: cosine
  grad_clip_norm: 1.0
  log_interval: 100
  eval_interval: 1000
  save_interval: 5000

# Data
data:
  dataset: wikitext
  dataset_config: wikitext-2-raw-v1
  train_split: train
  val_split: validation
  test_split: test
  num_workers: 4
  tokenizer_name: gpt2

# Optimization
optimizer:
  type: adamw
  betas: [0.9, 0.95]
  eps: 1.0e-8

# System
system:
  device: cuda
  mixed_precision: true
  compile: false
  seed: 42

# Logging
logging:
  use_wandb: false
  wandb_project: aletheion-baseline
  wandb_entity: null
  save_dir: ./checkpoints
  log_dir: ./logs
  run_name: null
