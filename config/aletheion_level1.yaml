# Aletheion Level 1 Configuration
# This config adds epistemic uncertainty quantification to the baseline transformer

# Model Architecture
model:
  vocab_size: 50257
  n_layers: 6
  n_heads: 8
  d_model: 512
  d_ff: 2048
  max_seq_len: 512
  dropout: 0.1
  use_flash_attention: false
  tie_weights: true

  # Epistemic Softmax Parameters (Aletheion Level 1)
  epistemic:
    # Q₁ gate threshold: confidence below this triggers temperature increase
    q1_threshold: 0.7

    # Q₂ gate threshold: cross-context consensus threshold
    q2_threshold: 0.7

    # Base temperature for softmax (τ₀ in paper)
    base_temperature: 1.0

    # Number of attention heads for Q₂ cross-attention
    n_consensus_heads: 4

    # VARO loss weight (λ in paper: L = L_CE + λ * ||u - u*||²)
    lambda_varo: 0.1

    # Method for computing target uncertainty u*:
    #   - 'head_variance': Use variance across attention heads (Phase 0-1)
    #   - 'data_ambiguity': Use label ambiguity (Phase 2)
    #   - 'uniform': Baseline uniform uncertainty
    u_star_method: head_variance

# Training
training:
  batch_size: 32
  gradient_accumulation_steps: 1
  learning_rate: 3.0e-4
  weight_decay: 0.1
  max_steps: 100000
  warmup_steps: 2000
  lr_schedule: cosine
  grad_clip_norm: 1.0
  log_interval: 100
  eval_interval: 1000
  save_interval: 5000

# Data
data:
  dataset: wikitext
  dataset_config: wikitext-2-raw-v1
  train_split: train
  val_split: validation
  test_split: test
  num_workers: 4
  tokenizer_name: gpt2

# Optimization
optimizer:
  type: adamw
  betas: [0.9, 0.95]
  eps: 1.0e-8

# System
system:
  device: cuda
  mixed_precision: true
  compile: false
  seed: 42

# Logging
logging:
  use_wandb: false
  wandb_project: aletheion-level1
  wandb_entity: null
  save_dir: ./checkpoints/aletheion_level1
  log_dir: ./logs/aletheion_level1
  run_name: aletheion_level1_baseline

# Notes:
# - This configuration extends the baseline with epistemic gates at output layer
# - For fair comparison with baseline, keep all non-epistemic hyperparameters identical
# - Adjust lambda_varo to control trade-off between perplexity and calibration:
#     * lambda_varo = 0.0 → baseline transformer (no uncertainty training)
#     * lambda_varo = 0.1 → balanced (recommended for Level 1)
#     * lambda_varo = 1.0 → strong uncertainty regularization (may hurt perplexity)
# - Expected overhead: < 1% computational cost vs baseline
