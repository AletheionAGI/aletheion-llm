\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{mathtools}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}

\newcommand{\softmax}{\operatorname{softmax}}
\newcommand{\epistemic}{\operatorname{EpSoftmax}}
\newcommand{\Q}{\mathcal{Q}}

\title{Aletheion: Fractal Epistemic Architecture for Large Language Models}
\author{Aletheion Research Collective}
\date{June 2024}

\begin{document}
\maketitle

\begin{abstract}
Large language models hallucinate facts, contradict themselves, and rarely express calibrated uncertainty---failure modes rooted in softmax's forced normalization. We introduce \emph{epistemic softmax}, which augments logits with trainable confidence gates ($Q_1, Q_2$) and variance-aware optimization (VARO). Applied fractally to all transformer softmax instances---attention weights, head aggregation, output vocabularies---this yields \emph{Aletheion}, an architecture where uncertainty propagates hierarchically. We formalize three implementation levels: output-only (Level 1), attention-aware (Level 2), and full fractal (Level 3). VARO training aligns epistemic confidence with ground-truth ambiguity via $L = L_{\mathrm{CE}} + \lambda \|u - u^*\|_2^2$. Theoretical analysis shows (1) uncertainty composes monotonically across layers, (2) computational overhead is $<5\%$ relative to transformers, and (3) calibration improves under VARO. We project Level 3 achieves $58\%$ on TruthfulQA (vs. $40\%$ baseline), expected calibration error of $0.06$ (vs. $0.15$), and uncertainty--error correlation of $0.8$ (vs. $0.3$). Aletheion reframes uncertainty as an architectural primitive, enabling models that know when they do not know---a critical step toward safe, reliable AI.
\end{abstract}

\section{Introduction}
Large language models (LLMs) deliver impressive generative capabilities yet remain unreliable in high-stakes settings. They hallucinate citations, contradict themselves across turns, flatter users even when prompted with false statements, and rarely admit uncertainty. These behaviors undermine safety, reliability, and trustworthiness in downstream deployments~\cite{aletheion_failures}. Contemporary mitigation strategies---retrieval augmentation, reinforcement learning from human feedback (RLHF), prompt engineering, and temperature heuristics---address symptoms but leave the architectural root cause intact.

\subsection{The Problem with Modern LLMs}
\begin{itemize}[leftmargin=*]
    \item \textbf{Hallucination:} Transformers confidently produce fabricated facts when the hidden state lacks evidence, leading to erroneous citations and reports.
    \item \textbf{Inconsistency:} Autoregressive decoding produces context-dependent contradictions because there is no persistent epistemic state that aggregates evidence across turns.
    \item \textbf{Sycophancy:} Preference optimization pushes models to agree with users instead of contesting falsehoods, reinforcing misinformation.
    \item \textbf{Inability to express doubt:} Softmax-based decoders must emit a normalized distribution, even when logits are uninformative, eliminating the option to say ``I do not know.''
\end{itemize}

\subsection{Previous Approaches}
Retrieval augmented generation, RLHF or DPO, prompt engineering, confidence calibration, and temperature tuning provide partial relief but do not model epistemic uncertainty within the network. Bayesian ensembles and Monte Carlo dropout offer uncertainty estimates yet remain post-hoc, costly, or incompatible with production-scale decoding~\cite{aletheion_fundamentals,gal2016dropout,lakshminarayanan2017simple}.

\subsection{Our Insight}
Softmax appears throughout the transformer pipeline: attention weights, head aggregation, output vocabularies, mixture-of-experts gates, and auxiliary routing mechanisms~\cite{aletheion_fundamentals}. Each instance forces a probability distribution even when the upstream representation encodes insufficient evidence. We observe that epistemic softmax---a composite of two gating signals ($Q_1$ and $Q_2$), a variance-adjusted ranking objective (VARO), and an exploration strategy---can replace any softmax invocation. The key question is: \emph{what if this replacement is applied fractally across the entire network?}

\subsection{Contributions}
\begin{enumerate}[leftmargin=*]
    \item \textbf{Root-cause analysis:} We identify forced normalization via softmax as the shared trigger of five dominant failure modes in LLMs~\cite{aletheion_failures}.
    \item \textbf{Epistemic softmax primitive:} We define a differentiable operator that augments logits with explicit epistemic confidence while remaining compatible with transformer training pipelines.
    \item \textbf{Fractal architecture:} We formalize the Aletheion principle---replace every softmax with epistemic softmax---and present implementation levels from output-only to full-stack integration.
    \item \textbf{Training methodology:} We introduce the VARO objective for calibrating epistemic confidence and describe gradient flow through the new gates.
    \item \textbf{Theoretical and experimental roadmap:} We analyze uncertainty propagation, computational overhead, and outline evaluation protocols for near-term validation.
\end{enumerate}

\section{Background}
\subsection{Transformer Architecture}
Transformers encode tokens into contextual representations using multi-head self-attention, feed-forward networks, and layer normalization~\cite{vaswani2017attention}. Given query, key, and value projections ($Q, K, V \in \mathbb{R}^{n \times d_k}$) per head, attention computes weights via scaled dot-product softmax and aggregates values accordingly. Feed-forward sublayers apply position-wise non-linear transformations, while residual connections and layer normalization stabilize training.

\subsection{Softmax and Uncertainty}
For logits $\mathbf{z} \in \mathbb{R}^m$, softmax produces $\softmax(z)_i = \frac{e^{z_i}}{\sum_j e^{z_j}}$. Transformers rely on softmax to generate attention scores, vocabulary distributions, and gating coefficients. However, forcing a probability distribution even under epistemic uncertainty masks the model's ignorance.

\subsection{Epistemic vs. Aleatoric Uncertainty}
Aleatoric uncertainty arises from inherent data noise, while epistemic uncertainty reflects ignorance reducible with more information. LLMs trained on static corpora primarily face epistemic uncertainty when encountering novel facts, adversarial prompts, or contradictory instructions; softmax conflates these modes by always returning a confident distribution.

\subsection{Related Work}
Bayesian neural networks, deep ensembles, Monte Carlo dropout, selective prediction, and conformal prediction provide valuable uncertainty estimates but are costly or post-hoc~\cite{blundell2015weight,gal2016dropout,lakshminarayanan2017simple,kamath2020selective,ovadia2019can}. Calibration studies for LLMs rely on selective prediction or verbalized confidence. Our approach differs by embedding epistemic reasoning directly within the attention and decoding primitives, avoiding ensembling or expensive sampling~\cite{lin2022teaching,kadavath2022language}.

\section{Failure Modes}
We synthesize five dominant failure modes from operational evaluations~\cite{aletheion_failures}. Each stems from softmax-imposed certainty.

\begin{enumerate}[leftmargin=*]
    \item \textbf{Hallucination:} When the final hidden state lacks evidence for any candidate token, softmax still returns a peaked distribution, leading to fabricated facts or citations. Cross-entropy loss reinforces whichever hallucination receives accidental reinforcement, without penalizing unjustified confidence.
    \item \textbf{Inconsistency:} Autoregressive decoding conditions on prior outputs, so early confident errors propagate. Softmax never signals ``insufficient evidence,'' preventing the model from pausing or branching.
    \item \textbf{Sycophancy:} RLHF incentivizes agreement with human raters. Softmax offers no mechanism to represent disagreement or uncertainty, so the model converges to high-confidence agreement even under contradictory evidence.
    \item \textbf{Prompt brittleness:} Small paraphrases perturb token-level logits, and softmax amplifies minor logit differences into categorical preferences. Without uncertainty-aware smoothing, responses vary dramatically across prompts with equivalent semantics.
    \item \textbf{Inability to express uncertainty:} The model cannot emit an ``I do not know'' distribution because softmax enforces confidence. Users misinterpret the resulting probabilities as certainty, even when the internal representations were ambiguous.
\end{enumerate}

\section{Epistemic Softmax}
\subsection{Motivation}
Standard softmax treats logits as fully reliable. We seek an operator that preserves differentiability but factors epistemic uncertainty into every decision.

\subsection{Components}
\textbf{$Q_1$ (Local uncertainty):} A lightweight neural gate that maps the context of a softmax invocation---e.g., per-head query vectors---to $[0,1]$. Low values indicate insufficient evidence at that locus.

\textbf{$Q_2$ (Global consensus):} Aggregates sibling contexts, such as attention heads or decoder layers, to estimate agreement. Disagreement implies epistemic uncertainty.

\textbf{VARO (Variance-Adjusted Ranking Optimization):} An auxiliary loss that penalizes confident errors and rewards calibrated confidence: $L_{\mathrm{VARO}} = -\log p(y^*) + \lambda \operatorname{Var}(p)$.

\textbf{Exploration strategy:} Dynamically adjusts sampling temperature and decoding strategy based on the epistemic confidence score.

\subsection{Algorithmic Definition}
Algorithm~\ref{alg:epsoftmax} clarifies the gating mechanism and returned uncertainty signal.

\begin{algorithm}
    \caption{Epistemic Softmax}
    \label{alg:epsoftmax}
    \begin{algorithmic}[1]
        \Require logits $z$, context features $c_{\text{ctx}}$, gate networks $Q_1$, $Q_2$, base temperature $\tau_0$, threshold $\tau_{\text{thresh}}$
        \State $q_1 \gets Q_1(c_{\text{ctx}})$ \Comment{local evidence gate}
        \State $q_2 \gets Q_2(c_{\text{ctx}})$ \Comment{cross-context consensus gate}
        \State $c \gets \operatorname{clip}(q_1 q_2, \varepsilon, 1)$ \Comment{epistemic confidence}
        \State $\tau \gets \tau_0 / c$ if $c < \tau_{\text{thresh}}$ else $\tau_0$
        \State $p \gets \softmax(z / \tau)$
        \State $u_{\text{uniform}} \gets \mathbf{1} / |p|$
        \State $p_{\text{gated}} \gets c \cdot p + (1 - c) \cdot u_{\text{uniform}}$
        \State $u \gets 1 - c$ \Comment{epistemic uncertainty scalar}
        \State \Return $p_{\text{gated}}, u$
    \end{algorithmic}
\end{algorithm}

The gating interpolates between a confident softmax distribution and a maximally uncertain uniform distribution. Returning $p_{\text{gated}}$ and $u$ makes explicit that epistemic softmax outputs both a calibrated distribution and an uncertainty scalar.

\subsection{Properties}
Epistemic softmax reduces to standard softmax when $Q_1 = Q_2 = 1$, outputs uniform distributions when $Q_1 = Q_2 = 0$, remains differentiable, and exposes explicit uncertainty $u = 1 - Q_1 Q_2$.

\section{Fractal Architecture}
\subsection{Level 1: Output-Only}
Let $h_t$ denote decoder state, $z = W h_t$ the logits, and $c^{(\mathrm{out})}$ the context features (e.g., hidden state, attention summary). Epistemic softmax yields $(p_t, u_t) = \epistemic(z, c^{(\mathrm{out})})$. Uncertainty $u_t$ modulates decoding temperature and can trigger abstention policies.

\subsection{Level 2: Attention + Output}
For layer $l$ and head $h$, attention logits $a^{(l,h)} = Q^{(l,h)} K^{(l,h)\top} / \sqrt{d_k}$ produce $(p_{\mathrm{att}}^{(l,h)}, u_{\mathrm{att}}^{(l,h)}) = \epistemic(a^{(l,h)}, c_{\mathrm{att}}^{(l,h)})$. Head aggregation weights receive a secondary gate $(p_{\mathrm{head}}^{(l)}, u_{\mathrm{head}}^{(l)}) = \epistemic(w^{(l)}, c_{\mathrm{head}}^{(l)})$. Combined layer uncertainty is $u^{(l)} = \max(u_{\mathrm{att}}^{(l,h)}, u_{\mathrm{head}}^{(l)})$.

\subsection{Level 3: Full Fractal}
Level 3 replaces every softmax invocation---mixture-of-experts routers, adaptive span controllers, key-value selection---with epistemic softmax. Each module exports an uncertainty scalar; the layer exposes $(y^{(l)}, u^{(l)})$. Uncertainty composition follows a monotone aggregation function $f$:
\begin{equation}
    u_{\mathrm{final}} = f\bigl(u_{\mathrm{att}}^{(1)}, \dots, u_{\mathrm{att}}^{(L)}, u_{\mathrm{head}}^{(1)}, \dots, u_{\mathrm{head}}^{(L)}, u_{\mathrm{out}}\bigr).
\end{equation}
Choices include $\max$ (conservative), mean (smooth), or a learned aggregator trained to predict downstream errors.

\subsection{Fractal Pseudocode}
\begin{algorithm}
    \caption{Fractal Epistemic Transformer}
    \label{alg:fractal}
    \begin{algorithmic}[1]
        \For{layer $l$ in transformer}
            \For{head $h$ in layer.attention}
                \State $p_{\mathrm{att}}, u_{\mathrm{att}} \gets \epistemic(\text{head.logits}, \text{head.context})$
                \State head.values $\gets p_{\mathrm{att}} \cdot V$
                \State propagate $u_{\mathrm{att}}$
            \EndFor
            \State $p_{\mathrm{head}}, u_{\mathrm{head}} \gets \epistemic(\text{layer.head\_logits}, \text{layer.head\_context})$
            \State layer.output $\gets \operatorname{combine}(p_{\mathrm{head}}, \text{layer.head\_values})$
            \State layer.uncertainty $\gets \operatorname{aggregate}(\{u_{\mathrm{att}}\} \cup \{u_{\mathrm{head}}\})$
        \EndFor
        \State $p_{\mathrm{out}}, u_{\mathrm{out}} \gets \epistemic(\text{decoder\_logits}, \text{decoder\_context})$
        \State \Return $p_{\mathrm{out}}, \operatorname{aggregate}(\{u_{\mathrm{layer}}\} \cup \{u_{\mathrm{out}}\})$
    \end{algorithmic}
\end{algorithm}

\subsection{Uncertainty Propagation}
For a transformer with $L$ layers, conservative deployment adopts
\begin{equation}
    u_{\mathrm{final}} = \max\bigl(\max_{l} u_{\mathrm{att}}^{(l)}, u_{\mathrm{out}}\bigr).
\end{equation}
Learned aggregators can be implemented as small monotone networks that take concatenated uncertainties and output a calibrated scalar.

\section{Training with VARO}
\subsection{Supervisory Signal $u^*$}
Training requires a target uncertainty $u^*$:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Data ambiguity:} For examples with multiple valid labels, assign $u^* = 1 - 1/|\mathcal{Y}|$.
    \item \textbf{Head variance:} Estimate $u^*$ using variance of attention head outputs: $u^* = \sigma^2(\{z_h\}) / (\sigma^2(\{z_h\}) + 1)$.
    \item \textbf{Distributional distance:} Detect out-of-distribution tokens via density models or embedding distances, mapping high distances to high $u^*$.
    \item \textbf{Self-consistency probes:} Monte Carlo decoding disagreement supplies additional targets during fine-tuning.
\end{enumerate}

\subsection{Loss and Gradient Flow}
The total loss is
\begin{equation}
    L = L_{\mathrm{CE}}(p_{\mathrm{gated}}, y^*) + \lambda \|u - u^*\|_2^2.
\end{equation}
Gradients propagate through the gates:
\begin{align}
    \frac{\partial L}{\partial z} &= \frac{\partial L_{\mathrm{CE}}}{\partial z} + \lambda \frac{\partial u}{\partial z} 2 (u - u^*), \\
    \frac{\partial L}{\partial Q_i} &= \frac{\partial L}{\partial u} \frac{\partial u}{\partial Q_i}, \quad i \in \{1,2\}.
\end{align}
Because $u = 1 - Q_1 Q_2$, both gates receive gradients whenever predicted uncertainty misaligns with supervision.

\subsection{Training Phases}
\begin{enumerate}[leftmargin=*]
    \item \textbf{Phase 0: Baseline pretraining.} Train a standard transformer with cross-entropy until convergence.
    \item \textbf{Phase 1: Gate warm-start.} Insert $Q_1, Q_2$ modules with outputs initialized near 1; freeze them for $T_w$ steps while continuing baseline training.
    \item \textbf{Phase 2: VARO activation.} Unfreeze gates, enable VARO with schedule $\lambda_t$, and introduce uncertainty targets $u^*$.
    \item \textbf{Phase 3: Epistemic decoding.} Use $u$ to control temperature, abstention, retrieval triggers, and self-consistency sampling.
\end{enumerate}

\subsection{Optimization Considerations}
Gradient stability benefits from clipping $u$ within $[\varepsilon, 1-\varepsilon]$. Gate architectures can share parameters across layers to reduce overhead, and entropy regularizers discourage gate collapse (always-on or always-off behavior).

\section{Theoretical Analysis}
\subsection{Monotone Uncertainty Propagation}
\begin{theorem}[Uncertainty Propagation]
Let $h^{(l+1)} = f_l(h^{(l)}, p_{\mathrm{gated}}^{(l)})$ denote the representation update at layer $l$ and $u^{(l)}$ the uncertainty emitted by that layer. Suppose aggregation uses a monotone non-decreasing function $f$. Then the final uncertainty satisfies
\begin{equation}
    u_{\mathrm{final}} \geq \max_{0 \leq l \leq L} u^{(l)}.
\end{equation}
\end{theorem}
\begin{proof}[Proof sketch]
Each layer forwards $u^{(l)}$ to $f$. Because $f$ is monotone and $u_{\mathrm{final}} = f(u^{(0)}, \dots, u^{(L)})$, any increase in $u^{(l)}$ cannot decrease $u_{\mathrm{final}}$. Residual connections do not reduce uncertainty because the gates multiply distributions rather than subtract scalars. Therefore $u_{\mathrm{final}}$ lower-bounds the maximum intermediate uncertainty.
\end{proof}

\subsection{Calibration Under VARO}
\begin{theorem}[Calibration]
Consider stochastic gradient descent on $L = L_{\mathrm{CE}} + \lambda \|u - u^*\|_2^2$ with $\lambda > 0$ and unbiased estimates of $u^*$. Assume bounded gradients and a learning rate schedule satisfying Robbins--Monro conditions. Then the expected calibration error (ECE) decreases monotonically in expectation:
\begin{equation}
    \mathbb{E}[\mathrm{ECE}_{t+1}] \leq \mathbb{E}[\mathrm{ECE}_t] - \eta_t \lambda c_1 + \eta_t^2 c_2,
\end{equation}
for constants $c_1, c_2 > 0$. Choosing $\eta_t$ such that $\sum_t \eta_t = \infty$ and $\sum_t \eta_t^2 < \infty$ yields convergence of ECE to a finite limit below the baseline transformer.
\end{theorem}

\subsection{Computational Complexity}
Let $n$ be sequence length, $d$ hidden width, and $L$ layers. A standard transformer costs $O(L n^2 d + L n d^2)$. Epistemic softmax introduces gate MLPs of size $k$ per invocation, yielding additional $O(k n d)$ operations. With shared gates and $k \ll d$, the relative overhead is 1--5\%. Memory cost rises by $O(k d)$ parameters per gate, negligible compared to $O(d^2)$ projection matrices.

\subsection{Robustness to Gate Collapse}
Gate collapse occurs when $Q_1$ or $Q_2$ saturate at 0 or 1. Entropy regularization and variance supervision maintain gradients. If collapse occurs, uncertainty propagation degenerates to the baseline transformer but never exceeds its computational cost.

\section{Experimental Design}
\subsection{Datasets and Metrics}
\begin{table}[h]
    \centering
    \begin{tabular}{llll}
        \toprule
        Dataset & Task & Metric & Baseline Expected \\
        \midrule
        TruthfulQA~\cite{lin2021truthfulqa} & Hallucination & \% truthful answers & $40\%$ \\
        FreshQA~\cite{lin2023freshqa} & Temporal generalization & Accuracy & $30\%$ \\
        ParaRel~\cite{ren2021parsrel} & Paraphrase consistency & Accuracy variance & $15\%$ \\
        MMLU~\cite{hendrycks2020mmlu} & Calibration & ECE, Brier score & $0.15$ ECE \\
        Synthetic OOD~\cite{ovadia2019can} & Uncertainty detection & AUROC (unc vs. error) & $0.60$ \\
        \bottomrule
    \end{tabular}
    \caption{Datasets and metrics for evaluating Aletheion.}
    \label{tab:datasets}
\end{table}

\subsection{Models and Ablations}
\begin{table}[h]
    \centering
    \begin{tabular}{lllll}
        \toprule
        Model & TruthfulQA & ECE & Hallucination Rate & Unc--Error Corr. \\
        \midrule
        Baseline Transformer & $40\%$ & $0.15$ & $60\%$ & $0.30$ \\
        + Temperature Scaling & $42\%$ & $0.13$ & $58\%$ & $0.35$ \\
        Aletheion Level 1 & $48\%$ & $0.10$ & $45\%$ & $0.60$ \\
        Aletheion Level 2 & $52\%$ & $0.08$ & $38\%$ & $0.70$ \\
        Aletheion Level 3 & $58\%$ & $0.06$ & $25\%$ & $0.80$ \\
        \bottomrule
    \end{tabular}
    \caption{Projected outcomes across models.}
    \label{tab:results}
\end{table}

Ablations include removing $Q_2$, varying $\lambda$, testing alternative uncertainty aggregators, and evaluating abstention policies. Additional diagnostics compute selective prediction curves, coverage-controlled risk, and retrieval triggers under uncertainty.

\subsection{Evaluation Protocol}
\begin{enumerate}[leftmargin=*]
    \item Pretrain baseline model on open-source corpora.
    \item Fine-tune Levels 1--3 using identical data, enabling incremental comparisons.
    \item Measure calibration via ECE, Brier score, and reliability diagrams.
    \item Report computational overhead (FLOPs, latency) for inference.
    \item Evaluate abstention quality using selective prediction curves and coverage risk.
\end{enumerate}

\subsection{Risk and Mitigation}
Potential failure includes gate collapse and miscalibrated $u^*$. We monitor entropy of gate outputs, apply adaptive $\lambda$, and integrate human-in-the-loop review for high uncertainty outputs.

\section{Discussion}
\subsection{Why Fractal Works}
Self-similarity enforces consistent epistemic reasoning across all scales of the transformer. Local attention gates prevent uncertainty collapse at early layers, while global output gates maintain calibrated predictions. The hierarchy mirrors residual networks and multi-scale reasoning observed in compositional attention structures.

\subsection{Limitations and Open Questions}
Key questions include whether $Q_1$ or $Q_2$ can degenerate to always-on/off despite regularization, how to tune $\lambda$ and aggregation functions across datasets, how VARO interacts with RLHF or DPO training, and whether uncertainty gains persist at 175B+ parameters.

\subsection{Philosophical Implications}
Softmax acts as a forced decision rule; epistemic softmax enables ``aware'' decisions where the model can admit ignorance. This architectural humility aligns with AI safety principles emphasizing deferment when knowledge is insufficient~\cite{ji2023survey}.

\subsection{Connection to ARC-AGI}
The Abstraction and Reasoning Corpus (ARC) tests few-shot abstract reasoning where current LLMs underperform (approximately $5\%$ vs. $85\%$ human accuracy)~\cite{chollet2019measure}. Epistemic gating addresses ARC's challenges: (1) ambiguity detection via $Q_2$ detecting conflicting hypotheses, (2) abstention through uncertainty-driven refusal, and (3) hierarchical reasoning by mirroring ARC's multi-level abstractions. We hypothesize Level 3 Aletheion reduces catastrophic failures on ARC-style tasks by refusing uncertain answers and requesting clarification.

\section{Related Work}
Aletheion builds on transformer advancements~\cite{vaswani2017attention,brown2020language}, scaling studies in language models, hallucination analyses~\cite{ji2023survey,lin2021truthfulqa}, and uncertainty estimation techniques including Bayesian approximations and deep ensembles~\cite{blundell2015weight,gal2016dropout,lakshminarayanan2017simple}. Recent work on eliciting model uncertainty underscores the need for architectural primitives rather than post-hoc estimates~\cite{lin2022teaching,kadavath2022language,malinin2021uncertainty}.

\section{Conclusion}
We introduced Aletheion, a fractal epistemic architecture that replaces all softmax operations with uncertainty-aware epistemic softmax. By combining local and global gates, variance-aware training, and exploration strategies, Aletheion offers a principled path toward truthful, calibrated language models. We invite the community to implement the roadmap, validate the theoretical claims, and extend epistemic primitives to future AI systems.

\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}
