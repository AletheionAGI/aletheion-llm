@misc{aletheion_fundamentals,
  title        = {Aletheion LLM Fundamentals},
  author       = {Aletheion Research Collective},
  year         = {2024},
  howpublished = {Internal documentation},
  note         = {\url{https://github.com/aletheion-llm}}
}

@misc{aletheion_failures,
  title        = {Operational Failure Modes of Large Language Models},
  author       = {Aletheion Research Collective},
  year         = {2024},
  howpublished = {Internal documentation},
  note         = {\url{https://github.com/aletheion-llm}}
}

@article{vaswani2017attention,
  title   = {Attention is All You Need},
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2017}
}

@article{blundell2015weight,
  title   = {Weight Uncertainty in Neural Networks},
  author  = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  journal = {International Conference on Machine Learning},
  year    = {2015}
}

@inproceedings{gal2016dropout,
  title     = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  author    = {Gal, Yarin and Ghahramani, Zoubin},
  booktitle = {International Conference on Machine Learning},
  year      = {2016}
}

@inproceedings{lakshminarayanan2017simple,
  title     = {Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
  author    = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2017}
}

@inproceedings{kamath2020selective,
  title     = {Selective Question Answering under Domain Shift},
  author    = {Kamath, Aditi and Jia, Robin and Liang, Percy},
  booktitle = {International Conference on Machine Learning},
  year      = {2020}
}

@article{lin2021truthfulqa,
  title   = {TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  author  = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal = {arXiv preprint arXiv:2109.07958},
  year    = {2021}
}

@article{ji2023survey,
  title   = {Survey of Hallucination in Natural Language Generation},
  author  = {Ji, Zhehui and Xu, Tianyi and Wang, Bo and Chao, Wei and Wong, Kam-Fai and Zha, Hongyuan and He, Xiaodong},
  journal = {ACM Computing Surveys},
  year    = {2023}
}

@article{perez2022discovering,
  title   = {Discovering Language Model Behaviors with Model-Written Evaluations},
  author  = {Perez, Ethan and Huang, He and Song, Frances Ann and Ba, Jimmy and Cai, Trevor},
  journal = {arXiv preprint arXiv:2212.09251},
  year    = {2022}
}

@inproceedings{wang2022self,
  title     = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author    = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  booktitle = {International Conference on Learning Representations},
  year      = {2023}
}

@article{brown2020language,
  title   = {Language Models are Few-Shot Learners},
  author  = {Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and others},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2020}
}

@article{chollet2019measure,
  title   = {On the Measure of Intelligence},
  author  = {Chollet, Fran\c{c}ois},
  journal = {arXiv preprint arXiv:1911.01547},
  year    = {2019}
}

@article{lin2022teaching,
  title   = {Teaching Models to Express Their Uncertainty},
  author  = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal = {arXiv preprint arXiv:2205.14334},
  year    = {2022}
}

@article{tu2023tempquestions,
  title   = {TempQuestions: A Benchmark for Temporal Question Answering},
  author  = {Tu, Shangqing and Yu, Jifan and Wang, Xiaozhi and Li, Juanzi and Hou, Lei},
  journal = {arXiv preprint arXiv:2305.17173},
  year    = {2023}
}

@article{elazar2021consistency,
  title   = {Measuring and Improving Consistency in Pretrained Language Models},
  author  = {Elazar, Yanai and Kassner, Nora and Ravfogel, Shauli and Ravichander, Abhilasha and Hovy, Eduard and Sch"utze, Hinrich and Goldberg, Yoav},
  journal = {Transactions of the Association for Computational Linguistics},
  volume  = {9},
  pages   = {1012--1031},
  year    = {2021}
}

@inproceedings{guo2017calibration,
  title     = {On Calibration of Modern Neural Networks},
  author    = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle = {International Conference on Machine Learning},
  pages     = {1321--1330},
  year      = {2017}
}

@article{hendrycks2020mmlu,
  title   = {Measuring Massive Multitask Language Understanding},
  author  = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal = {International Conference on Learning Representations},
  year    = {2021}
}

@article{ovadia2019can,
  title   = {Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty under Dataset Shift},
  author  = {Ovadia, Yaniv and others},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2019}
}

@article{malinin2021uncertainty,
  title   = {Uncertainty Estimation in Autoregressive Sequence Models},
  author  = {Malinin, Andrey and others},
  journal = {International Conference on Learning Representations},
  year    = {2021}
}

@article{jiang2021know,
  title   = {Know When to Stop: Evaluation and Calibration of Termination Prediction},
  author  = {Jiang, Heinrich and Gupta, Manzil and Wei, Jason and He, He and Liang, Percy},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2021}
}

@article{kadavath2022language,
  title   = {Language Models (Mostly) Know What They Know},
  author  = {Kadavath, Saurav and others},
  journal = {arXiv preprint arXiv:2207.05221},
  year    = {2022}
}

