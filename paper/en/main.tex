\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{mathtools}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\epsoftmax}{\mathrm{EpSoftmax}}
\newcommand{\Q}{\mathcal{Q}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{How to Solve Skynet: A Pyramidal Law for Epistemic Equilibrium}
\author{Aletheion Research Collective}
\date{June 2024}

\begin{document}
\maketitle

% Copyright notice
\vspace{-0.5em}
\noindent
\textcopyright~2024 Felipe Maya Muniz. All rights reserved.
\vspace{0.5em}

\begin{abstract}
    The Skynet problem—AI systems becoming increasingly overconfident as they scale—poses a fundamental threat to deployment safety.
    We present a geometric solution: a pyramidal architecture with five irreducible components.
    Its four-dimensional base simplex encodes the forces of Memory, Pain, Choice, and Exploration; two epistemic gates distinguish aleatoric uncertainty ($Q_1$) from epistemic uncertainty ($Q_2$); a derived Height coordinate measures proximity to truth; and an Apex vertex at 1.0 represents absolute truth.
    
    Without explicit epistemic gates, models drift toward apex delusion: in our baseline pyramidal architecture, Height reached 1.000 while Expected Calibration Error (ECE) degraded from 0.011 to 0.084—7.6× worse.
    The gated Q1Q2 pyramidal model prevents this collapse, achieving ECE = 0.060 and controlled Height = 0.971 within only 5,000 steps—an 89 \% calibration improvement over the ungated endpoint, maintaining a Height/ECE ratio of 16.2.
    These results demonstrate that safe AGI requires geometric constraints, not merely scale: a stable foundation and an invariant reference point encoded architecturally.
    
    The system exhibits signs of decisional consistency and reduced exploratory entropy, analogous to an emergent cognitive style—yet remains confined to the domain of optimization.
\end{abstract}

\section*{Notation}

Throughout this paper, we use the following conventions:

\subsection*{Dimensions}
\begin{itemize}
    \item $L$: number of transformer layers
    \item $H$: number of attention heads per layer
    \item $d$: model hidden dimension ($d_{\text{model}}$)
    \item $d_k$: dimension of keys/queries (typically $d/H$)
    \item $d_v$: dimension of values (typically $d/H$)
    \item $n$: sequence length
    \item $V$: vocabulary size
    \item $k$: gate MLP hidden dimension
\end{itemize}

\subsection*{Variables}
\begin{itemize}
    \item $h^{(l)} \in \mathbb{R}^{n \times d}$: hidden state at layer $l$
    \item $Q^{(l,h)}, K^{(l,h)}, V^{(l,h)} \in \mathbb{R}^{n \times d_k}$: query, key, value matrices for head $h$ of layer $l$
    \item $a^{(l,h)} \in \mathbb{R}^{n \times n}$: attention logits for head $h$ of layer $l$
    \item $p^{(l,h)}_{\text{att}} \in \mathbb{R}^{n \times n}$: attention weights after epistemic softmax
    \item $u^{(l,h)}_{\text{att}} \in [0,1]$: uncertainty from attention head $h$ of layer $l$
    \item $o^{(l,h)} \in \mathbb{R}^{n \times d_v}$: output of attention head $h$ in layer $l$
    \item $w^{(l)} \in \mathbb{R}^{H}$: head aggregation logits at layer $l$
    \item $u^{(l)} \in [0,1]$: aggregated uncertainty from layer $l$
    \item $u_{\text{final}} \in [0,1]$: final output uncertainty
\end{itemize}

\subsection*{Functions}
\begin{itemize}
    \item $Q_1: \mathbb{R}^d \to [0,1]$: local uncertainty gate
    \item $Q_2: \mathbb{R}^d \to [0,1]$: global consensus gate
    \item $\texttt{EpSoftmax}$: epistemic softmax operator (Algorithm 1)
    \item $f: [0,1]^{L+1} \to [0,1]$: uncertainty aggregation function
\end{itemize}

\subsection*{Pyramidal Components}
\begin{itemize}
    \item $\mathbf{b} \in \Delta^3$: base simplex vector $(w_{\text{memory}}, w_{\text{pain}}, w_{\text{choice}}, w_{\text{exploration}})$
    \item $Q_1 \in [0,1]$: aleatoric uncertainty (irreducible, data noise)
    \item $Q_2 \in [0,1]$: epistemic uncertainty (reducible, model ignorance)
    \item $h \in [0,1]$: height coordinate (derived from $Q_1$, $Q_2$, base stability)
    \item $\text{apex} = 1.0$: truth vertex (constant attractor)
    \item $\sigma_{Q_1}^2, \sigma_{Q_2}^2$: fractal variances (uncertainty about uncertainty)
    \item $u_{\text{fractal}}$: meta-epistemic uncertainty
    \item $U_{\text{total}} = Q_1 + Q_2 \cdot (1 + u_{\text{fractal}})$: total uncertainty
\end{itemize}

\subsection*{Losses}
\begin{itemize}
    \item $\mathcal{L}_{\text{CE}}$: cross-entropy loss
    \item $\mathcal{L}_{\text{base}}$: base stability regularization
    \item $\mathcal{L}_{Q_1}$: aleatoric uncertainty calibration
    \item $\mathcal{L}_{Q_2}$: epistemic uncertainty calibration
    \item $\mathcal{L}_{\text{fractal}}$: fractal variance regularization
    \item $\mathcal{L}_{\text{height}}$: height derivation consistency
    \item $\mathcal{L} = \mathcal{L}_{\text{CE}} + \lambda_{\text{base}} \mathcal{L}_{\text{base}} + \lambda_{Q_1} \mathcal{L}_{Q_1} + \lambda_{Q_2} \mathcal{L}_{Q_2} + \lambda_{\text{fractal}} \mathcal{L}_{\text{fractal}} + \lambda_{\text{height}} \mathcal{L}_{\text{height}}$
\end{itemize}

\section*{Theoretical Foundation}

The epistemic framework underlying Aletheion originates
from the author's Quality Function of Truth, formalized
in a manuscript submitted to \textit{Episteme: A Journal
of Individual and Social Epistemology} \cite{muniz2024quality}.

That work proposed that truth manifests through varying 
degrees of symbolic fidelity $q(s) = F(\psi_s, T)$, where 
consciousness evolves its capacity to reflect absolute 
truth $T$ through representational states $\psi_s$ within 
a stratified field of cognition $\Phi$.

Aletheion represents the computational realization of this 
framework: epistemic uncertainty in language models mirrors 
the variable fidelity with which consciousness translates 
invariant truth. The Q$_1$ gate quantifies local uncertainty 
(analogous to $\psi_s$), while Q$_2$ captures cross-context 
consensus (analogous to field $\Phi$), together modulating 
temperature adaptively based on epistemic confidence.

This work thus bridges philosophical epistemology and 
machine learning, demonstrating that uncertainty quantification 
in AI can be grounded in formal theories of truth and cognition.

% Main content sections
\input{introduction}
\input{background}
\input{failure_modes}
\input{pyramidal_architecture}
\input{epistemic_softmax}
\input{fractal_architecture}
\input{training_varo}
\input{adaptive_epistemic_dynamics}
\input{theoretical_analysis}
\input{experimental_design}
\input{discussion}
\input{related_work}
\input{conclusion}

\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}
