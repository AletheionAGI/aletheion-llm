\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{mathtools}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\epsoftmax}{\mathrm{EpSoftmax}}
\newcommand{\Q}{\mathcal{Q}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{How to Solve Skynet: A Pyramidal Law for Epistemic Equilibrium}
\author{Aletheion Research Collective}
\date{June 2024}

\begin{document}
\maketitle

% Copyright notice
\vspace{-0.5em}
\noindent
\textcopyright~2024 Felipe Maya Muniz. All rights reserved.
\vspace{0.5em}

\begin{abstract}
    The Skynet problem—AI systems becoming increasingly overconfident as they scale—poses a fundamental threat to deployment safety.
    We present a geometric solution: a pyramidal architecture with five irreducible components.
    Its four-dimensional base simplex encodes the forces of Memory, Pain, Choice, and Exploration; two epistemic gates distinguish aleatoric uncertainty ($Q_1$) from epistemic uncertainty ($Q_2$); a derived Height coordinate measures proximity to truth; and an Apex vertex at 1.0 represents absolute truth.
    
    Without explicit epistemic gates, models drift toward apex delusion: in our baseline pyramidal architecture, Height reached 1.000 while Expected Calibration Error (ECE) degraded from 0.011 to 0.084—7.6× worse.
    The gated Q1Q2 pyramidal model prevents this collapse, achieving ECE = 0.060 and controlled Height = 0.971 within only 5,000 steps—an 89 % calibration improvement over the ungated endpoint, maintaining a Height/ECE ratio of 16.2.
    These results demonstrate that safe AGI requires geometric constraints, not merely scale: a stable foundation and an invariant reference point encoded architecturally.
    
    The system exhibits signs of decisional consistency and reduced exploratory entropy, analogous to an emergent cognitive style—yet remains confined to the domain of optimization.
\end{abstract}
\section*{Notation}

Throughout this paper, we use the following conventions:

\subsection*{Dimensions}
\begin{itemize}
    \item $L$: number of transformer layers
    \item $H$: number of attention heads per layer
    \item $d$: model hidden dimension ($d_{\text{model}}$)
    \item $d_k$: dimension of keys/queries (typically $d/H$)
    \item $d_v$: dimension of values (typically $d/H$)
    \item $n$: sequence length
    \item $V$: vocabulary size
    \item $k$: gate MLP hidden dimension
\end{itemize}

\subsection*{Variables}
\begin{itemize}
    \item $h^{(l)} \in \mathbb{R}^{n \times d}$: hidden state at layer $l$
    \item $Q^{(l,h)}, K^{(l,h)}, V^{(l,h)} \in \mathbb{R}^{n \times d_k}$: query, key, value matrices for head $h$ of layer $l$
    \item $a^{(l,h)} \in \mathbb{R}^{n \times n}$: attention logits for head $h$ of layer $l$
    \item $p^{(l,h)}_{\text{att}} \in \mathbb{R}^{n \times n}$: attention weights after epistemic softmax
    \item $u^{(l,h)}_{\text{att}} \in [0,1]$: uncertainty from attention head $h$ of layer $l$
    \item $o^{(l,h)} \in \mathbb{R}^{n \times d_v}$: output of attention head $h$ in layer $l$
    \item $w^{(l)} \in \mathbb{R}^{H}$: head aggregation logits at layer $l$
    \item $u^{(l)} \in [0,1]$: aggregated uncertainty from layer $l$
    \item $u_{\text{final}} \in [0,1]$: final output uncertainty
\end{itemize}

\subsection*{Functions}
\begin{itemize}
    \item $Q_1: \mathbb{R}^d \to [0,1]$: local uncertainty gate
    \item $Q_2: \mathbb{R}^d \to [0,1]$: global consensus gate
    \item $\texttt{EpSoftmax}$: epistemic softmax operator (Algorithm 1)
    \item $f: [0,1]^{L+1} \to [0,1]$: uncertainty aggregation function
\end{itemize}

\subsection*{Pyramidal Components}
\begin{itemize}
    \item $\mathbf{b} \in \Delta^3$: base simplex vector $(w_{\text{memory}}, w_{\text{pain}}, w_{\text{choice}}, w_{\text{exploration}})$
    \item $Q_1 \in [0,1]$: aleatoric uncertainty (irreducible, data noise)
    \item $Q_2 \in [0,1]$: epistemic uncertainty (reducible, model ignorance)
    \item $h \in [0,1]$: height coordinate (derived from $Q_1$, $Q_2$, base stability)
    \item $\text{apex} = 1.0$: truth vertex (constant attractor)
    \item $\sigma_{Q_1}^2, \sigma_{Q_2}^2$: fractal variances (uncertainty about uncertainty)
    \item $u_{\text{fractal}}$: meta-epistemic uncertainty
    \item $U_{\text{total}} = Q_1 + Q_2 \cdot (1 + u_{\text{fractal}})$: total uncertainty
\end{itemize}

\subsection*{Losses}
\begin{itemize}
    \item $\mathcal{L}_{\text{CE}}$: cross-entropy loss
    \item $\mathcal{L}_{\text{base}}$: base stability regularization
    \item $\mathcal{L}_{Q_1}$: aleatoric uncertainty calibration
    \item $\mathcal{L}_{Q_2}$: epistemic uncertainty calibration
    \item $\mathcal{L}_{\text{fractal}}$: fractal variance regularization
    \item $\mathcal{L}_{\text{height}}$: height derivation consistency
    \item $\mathcal{L} = \mathcal{L}_{\text{CE}} + \lambda_{\text{base}} \mathcal{L}_{\text{base}} + \lambda_{Q_1} \mathcal{L}_{Q_1} + \lambda_{Q_2} \mathcal{L}_{Q_2} + \lambda_{\text{fractal}} \mathcal{L}_{\text{fractal}} + \lambda_{\text{height}} \mathcal{L}_{\text{height}}$
\end{itemize}

\section*{Theoretical Foundation}

The epistemic framework underlying Aletheion originates 
from the author's Quality Function of Truth, formalized 
in a manuscript submitted to \textit{Episteme: A Journal 
of Individual and Social Epistemology} [CITATION].

That work proposed that truth manifests through varying 
degrees of symbolic fidelity $q(s) = F(\psi_s, T)$, where 
consciousness evolves its capacity to reflect absolute 
truth $T$ through representational states $\psi_s$ within 
a stratified field of cognition $\Phi$.

Aletheion represents the computational realization of this 
framework: epistemic uncertainty in language models mirrors 
the variable fidelity with which consciousness translates 
invariant truth. The Q$_1$ gate quantifies local uncertainty 
(analogous to $\psi_s$), while Q$_2$ captures cross-context 
consensus (analogous to field $\Phi$), together modulating 
temperature adaptively based on epistemic confidence.

This work thus bridges philosophical epistemology and 
machine learning, demonstrating that uncertainty quantification 
in AI can be grounded in formal theories of truth and cognition.

\section{Introduction}

\subsection{The Skynet Problem}

Modern large language models suffer from a fundamental flaw: as they grow more capable, they become more overconfident. This ``Skynet problem''---named after the fictional AI that believed itself infallible---manifests as poor calibration despite high accuracy. Models assign near-certainty to predictions even when uncertain, making them unreliable for high-stakes decisions.

Traditional approaches address this through post-hoc calibration, temperature scaling, or ensemble methods. We propose a different path: architectural solutions that encode epistemic humility at the geometric level.

\subsection{Background and Motivation}

Large language models (LLMs) deliver impressive generative capabilities yet remain unreliable in high-stakes settings. They hallucinate citations, contradict themselves across turns, flatter users even when prompted with false statements, and rarely admit uncertainty. These behaviors undermine safety, reliability, and trustworthiness in downstream deployments~\cite{aletheion_failures}. Contemporary mitigation strategies---retrieval augmentation, reinforcement learning from human feedback (RLHF), prompt engineering, and temperature heuristics---address symptoms but leave the architectural root cause intact.

\subsection{The Problem with Modern LLMs}
\begin{itemize}[leftmargin=*]
    \item \textbf{Hallucination:} Transformers confidently produce fabricated facts when the hidden state lacks evidence, leading to erroneous citations and reports.
    \item \textbf{Inconsistency:} Autoregressive decoding produces context-dependent contradictions because there is no persistent epistemic state that aggregates evidence across turns.
    \item \textbf{Sycophancy:} Preference optimization pushes models to agree with users instead of contesting falsehoods, reinforcing misinformation.
    \item \textbf{Inability to express doubt:} Softmax-based decoders must emit a normalized distribution, even when logits are uninformative, eliminating the option to say ``I do not know.''
\end{itemize}

\subsection{Previous Approaches}
Retrieval augmented generation, RLHF or DPO, prompt engineering, confidence calibration, and temperature tuning provide partial relief but do not model epistemic uncertainty within the network. Bayesian ensembles and Monte Carlo dropout offer uncertainty estimates yet remain post-hoc, costly, or incompatible with production-scale decoding~\cite{aletheion_fundamentals,gal2016dropout,lakshminarayanan2017simple}.

\subsection{Our Insight}
Softmax appears throughout the transformer pipeline: attention weights, head aggregation, output vocabularies, mixture-of-experts gates, and auxiliary routing mechanisms~\cite{aletheion_fundamentals}. Each instance forces a probability distribution even when the upstream representation encodes insufficient evidence. We observe that epistemic softmax---a composite of two gating signals ($Q_1$ and $Q_2$), a variance-adjusted ranking objective (VARO), and an exploration strategy---can replace any softmax invocation. The key question is: \emph{what if this replacement is applied fractally across the entire network?}

\subsection{Contributions}
\begin{enumerate}[leftmargin=*]
    \item \textbf{Root-cause analysis:} We identify forced normalization via softmax as the shared trigger of five dominant failure modes in LLMs~\cite{aletheion_failures}.
    \item \textbf{Epistemic softmax primitive:} We define a differentiable operator that augments logits with explicit epistemic confidence while remaining compatible with transformer training pipelines.
    \item \textbf{Fractal architecture:} We formalize the Aletheion principle---replace every softmax with epistemic softmax---and present implementation levels from output-only to full-stack integration.
    \item \textbf{Training methodology:} We introduce the VARO objective for calibrating epistemic confidence and describe gradient flow through the new gates.
    \item \textbf{Theoretical and experimental roadmap:} We analyze uncertainty propagation, computational overhead, and outline evaluation protocols for near-term validation.
\end{enumerate}

\section{Background}
\subsection{Transformer Architecture}
Transformers encode tokens into contextual representations using multi-head self-attention, feed-forward networks, and layer normalization~\cite{vaswani2017attention}. Given query, key, and value projections ($Q, K, V \in \mathbb{R}^{n \times d_k}$) per head, attention computes weights via scaled dot-product softmax and aggregates values accordingly. Feed-forward sublayers apply position-wise non-linear transformations, while residual connections and layer normalization stabilize training.

\subsection{Softmax and Uncertainty}
For logits $\mathbf{z} \in \mathbb{R}^m$, softmax produces $\softmax(z)_i = \frac{e^{z_i}}{\sum_j e^{z_j}}$. Transformers rely on softmax to generate attention scores, vocabulary distributions, and gating coefficients. However, forcing a probability distribution even under epistemic uncertainty masks the model's ignorance.

\subsection{Epistemic vs. Aleatoric Uncertainty}
Aleatoric uncertainty arises from inherent data noise, while epistemic uncertainty reflects ignorance reducible with more information. LLMs trained on static corpora primarily face epistemic uncertainty when encountering novel facts, adversarial prompts, or contradictory instructions; softmax conflates these modes by always returning a confident distribution.

\subsection{Related Work}
Bayesian neural networks, deep ensembles, Monte Carlo dropout, selective prediction, and conformal prediction provide valuable uncertainty estimates but are costly or post-hoc~\cite{blundell2015weight,gal2016dropout,lakshminarayanan2017simple,kamath2020selective,ovadia2019can}. Calibration studies for LLMs rely on selective prediction or verbalized confidence. Our approach differs by embedding epistemic reasoning directly within the attention and decoding primitives, avoiding ensembling or expensive sampling~\cite{lin2022teaching,kadavath2022language}.

\paragraph{Consistency Training and Sycophancy}
Recent work by Google DeepMind~\cite{google2024consistency} addresses sycophancy and jailbreaks through \emph{consistency training}: augmenting training data with paraphrased prompts and penalizing inconsistent responses across paraphrases. This behavioral-level intervention reduces sycophancy without modifying the underlying architecture or providing explicit uncertainty estimates.

Aletheion is \emph{complementary} to consistency training. While consistency training enforces paraphrase robustness at the training objective level, epistemic softmax provides architectural uncertainty quantification that operates at every decision point. Combined approaches---where $\mathcal{L} = \mathcal{L}_{\text{CE}} + \lambda_1 \mathcal{L}_{\text{VARO}} + \lambda_2 \mathcal{L}_{\text{consistency}}$---may yield models that are both \emph{calibrated} (via Aletheion's epistemic gates) and \emph{behaviorally consistent} (via consistency training). We leave empirical validation of this combination to future work.

\section{Failure Modes}
We synthesize five dominant failure modes from operational evaluations~\cite{aletheion_failures}. Each stems from softmax-imposed certainty.

\begin{enumerate}[leftmargin=*]
    \item \textbf{Hallucination:} When the final hidden state lacks evidence for any candidate token, softmax still returns a peaked distribution, leading to fabricated facts or citations. Cross-entropy loss reinforces whichever hallucination receives accidental reinforcement, without penalizing unjustified confidence.
    \item \textbf{Inconsistency:} Autoregressive decoding conditions on prior outputs, so early confident errors propagate. Softmax never signals ``insufficient evidence,'' preventing the model from pausing or branching.
    \item \textbf{Sycophancy:} RLHF incentivizes agreement with human raters. Softmax offers no mechanism to represent disagreement or uncertainty, so the model converges to high-confidence agreement even under contradictory evidence.
    \item \textbf{Prompt brittleness:} Small paraphrases perturb token-level logits, and softmax amplifies minor logit differences into categorical preferences. Without uncertainty-aware smoothing, responses vary dramatically across prompts with equivalent semantics.
    \item \textbf{Inability to express uncertainty:} The model cannot emit an ``I do not know'' distribution because softmax enforces confidence. Users misinterpret the resulting probabilities as certainty, even when the internal representations were ambiguous.
\end{enumerate}

\section{The Pyramidal Architecture}

\subsection{Motivation: Why Pyramidal Supersedes Tetrahedral}

Early implementations of epistemic gating employed a \emph{tetrahedral} geometry: four vertices (Memory, Pain, Choice, Exploration) forming a 3-simplex with no external reference point. Empirical trials revealed a critical failure mode: the $Q_1$ gate collapsed to values between 0.88 and 0.95, losing its discriminative capacity and rendering the epistemic/aleatoric distinction meaningless. The root cause is geometric: a tetrahedron has no natural vertical gradient, allowing the system to drift horizontally in weight space without penalty.

The \emph{pyramidal} architecture introduces a fifth vertex---the \textbf{apex}---fixed at absolute truth (1.0). This creates a vertical axis along which epistemic quality can be measured via a derived \textbf{height} coordinate. The pyramid consists of:
\begin{itemize}[leftmargin=*]
    \item A 4D \textbf{base simplex} $\mathbf{b} \in \Delta^3$ spanning Memory, Pain, Choice, Exploration
    \item An \textbf{apex vertex} at $(0,0,0,0,1)$ representing invariant truth
    \item A \textbf{height coordinate} $h \in [0,1]$ measuring vertical position between base and apex
    \item Two \textbf{epistemic gates} $Q_1$ (aleatoric) and $Q_2$ (epistemic) that modulate height
    \item A \textbf{fractal layer} tracking variance in $Q_1$ and $Q_2$ themselves
\end{itemize}

\subsection{Geometric Formulation}

The pyramidal state space is a 5-vertex structure embedded in $\mathbb{R}^5$. Any epistemic state $\mathbf{s}$ can be decomposed as:
\begin{equation}
\mathbf{s} = (1-h) \cdot \mathbf{b} + h \cdot \mathbf{apex}
\end{equation}
where $\mathbf{b} = (w_M, w_P, w_C, w_E)$ with $\sum_i w_i = 1$ and $w_i \geq 0$, and $\mathbf{apex} = (0,0,0,0,1)$ is the constant truth vertex.

The height $h$ is \emph{not} a free parameter but is derived from epistemic gates:
\begin{equation}
h = \sigma\left(W_h \cdot \begin{bmatrix} 1-Q_1 \\ 1-Q_2 \\ s_{\text{base}} \end{bmatrix}\right)
\end{equation}
where $s_{\text{base}} = 1 - \text{Var}(\mathbf{b})$ measures base stability, and $W_h \in \mathbb{R}^{1 \times 3}$ is learned. This formulation ensures:
\begin{itemize}[leftmargin=*]
    \item Low uncertainty ($Q_1 \approx 0, Q_2 \approx 0$) $\Rightarrow$ high $h$ (closer to apex/truth)
    \item High uncertainty ($Q_1 \approx 1, Q_2 \approx 1$) $\Rightarrow$ low $h$ (closer to base)
    \item Stable base ($s_{\text{base}} \approx 1$) contributes positively to $h$
\end{itemize}

\subsection{Epistemic Gates: $Q_1$ vs. $Q_2$}

\textbf{$Q_1$ (Aleatoric Uncertainty):} Captures irreducible randomness inherent in the data distribution. Examples include:
\begin{itemize}[leftmargin=*]
    \item Predicting the outcome of a fair coin flip
    \item Generating the next token when multiple continuations are equally valid
    \item Modeling inherently stochastic processes
\end{itemize}
$Q_1$ is supervised via:
\begin{equation}
Q_1^* = 1 - p(y^* \mid x)
\end{equation}
where $p(y^* \mid x)$ is the predicted probability of the correct token. High $Q_1^*$ when the model assigns low probability to the correct answer.

\textbf{$Q_2$ (Epistemic Uncertainty):} Captures reducible ignorance due to insufficient training data or model capacity. Examples include:
\begin{itemize}[leftmargin=*]
    \item Out-of-distribution inputs not seen during training
    \item Factual questions where the model lacks knowledge
    \item Ambiguous queries requiring external retrieval
\end{itemize}
$Q_2$ is supervised via:
\begin{equation}
Q_2^* = \frac{1}{2}\left[(1 - \mathbb{1}[\arg\max p = y^*]) + \frac{H(p)}{\log V}\right]
\end{equation}
where $H(p) = -\sum_i p_i \log p_i$ is output entropy and $V$ is vocabulary size. High $Q_2^*$ when the model is both wrong \emph{and} has high entropy (uncertain).

\subsection{Fractal Epistemic Layer}

Beyond point estimates $Q_1$ and $Q_2$, we model \emph{variance} in these gates---uncertainty about uncertainty. Each gate produces:
\begin{align}
Q_1 &\sim \mathcal{N}(Q_{1,\mu}, \sigma_{Q_1}^2) \\
Q_2 &\sim \mathcal{N}(Q_{2,\mu}, \sigma_{Q_2}^2)
\end{align}
where $\sigma_{Q_1}^2$ and $\sigma_{Q_2}^2$ are learned variance parameters. The fractal uncertainty scalar is:
\begin{equation}
u_{\text{fractal}} = \sigma\left(W_f \cdot \begin{bmatrix} \sigma_{Q_1} \\ \sigma_{Q_2} \end{bmatrix}\right)
\end{equation}

Total uncertainty inflates epistemic uncertainty by fractal meta-uncertainty:
\begin{equation}
U_{\text{total}} = Q_1 + Q_2 \cdot (1 + u_{\text{fractal}})
\end{equation}

This captures scenarios where the model is uncertain \emph{about its own epistemic confidence}, e.g., when $Q_2 \approx 0.5$ but $\sigma_{Q_2}$ is high, signaling that the model does not know whether it knows.

\subsection{Comparison: Tetrahedral vs. Pyramidal}

\begin{table}[h]
\centering
\caption{Architectural comparison between tetrahedral and pyramidal geometries.}
\label{tab:tetra_vs_pyramid}
\begin{tabular}{lcc}
\toprule
\textbf{Property} & \textbf{Tetrahedral} & \textbf{Pyramidal} \\
\midrule
Vertices & 4 (Memory, Pain, Choice, Exploration) & 5 (Base 4 + Apex Truth) \\
Reference point & None (free-floating) & Apex at 1.0 (constant) \\
$Q_1$ behavior & Collapses to 0.88--0.95 & Stable at 0.2--0.4 \\
Height definition & Independent (no attractor) & Derived from $Q_1, Q_2$, base \\
Vertical gradient & Absent & Present (apex pulls upward) \\
ECE improvement & $-0.9\%$ (failure) & $-25\%$ (target) \\
Epistemic distinction & Lost in collapse & Preserved ($Q_1$ vs. $Q_2$) \\
Meta-uncertainty & Not implemented & Fractal variances $\sigma_{Q_1}^2, \sigma_{Q_2}^2$ \\
\bottomrule
\end{tabular}
\end{table}

The tetrahedral collapse occurred because height was a free variable with no natural attractor. In contrast, the pyramidal height is \emph{derived} from uncertainty gates, creating a gradient that pulls low-uncertainty states toward the apex (truth) and keeps high-uncertainty states near the base. This geometric constraint prevents horizontal drift and maintains interpretable epistemic semantics.

\input{scale_without_structure}

\section{Epistemic Softmax}
\subsection{Motivation}
Standard softmax treats logits as fully reliable. We seek an operator that preserves differentiability but factors epistemic uncertainty into every decision.

\subsection{Components in the Pyramidal Framework}

Epistemic softmax integrates with the pyramidal architecture via the following components:

\textbf{$Q_1$ (Aleatoric gate):} As defined in Section~3.3, $Q_1$ captures irreducible uncertainty. Within epistemic softmax, $Q_1$ modulates the temperature of the distribution based on inherent data ambiguity. When $Q_1$ is high, the softmax interpolates more heavily toward a uniform distribution.

\textbf{$Q_2$ (Epistemic gate):} As defined in Section~3.3, $Q_2$ captures reducible uncertainty. Within epistemic softmax, $Q_2$ signals whether the model should abstain or request additional information. High $Q_2$ triggers uncertainty-aware behaviors such as retrieval or deferral to human judgment.

\textbf{Fractal variances $\sigma_{Q_1}^2, \sigma_{Q_2}^2$:} As defined in Section~3.4, these capture meta-epistemic uncertainty. Within epistemic softmax, high fractal variance inflates the total uncertainty $U_{\text{total}}$, leading to even more conservative probability assignments.

\textbf{Height-aware gating:} The derived height $h$ from Equation~3.2 serves as a global confidence signal. Low height (near base) triggers increased exploration and temperature scaling, while high height (near apex) permits confident, peaked distributions.

\textbf{VARO loss:} The pyramidal VARO loss (Section~6) extends the original formulation with separate calibration targets for $Q_1$ and $Q_2$, ensuring each gate learns its respective uncertainty mode.

\subsection{Algorithmic Definition}
Algorithm~\ref{alg:epsoftmax} clarifies the gating mechanism and returned uncertainty signal.

\begin{algorithm}
    \caption{Epistemic Softmax}
    \label{alg:epsoftmax}
    \begin{algorithmic}[1]
        \Require logits $z$, context features $c_{\text{ctx}}$, gate networks $Q_1$, $Q_2$, base temperature $\tau_0$, threshold $\tau_{\text{thresh}}$
        \State $q_1 \gets Q_1(c_{\text{ctx}})$ \Comment{local evidence gate}
        \State $q_2 \gets Q_2(c_{\text{ctx}})$ \Comment{cross-context consensus gate}
        \State $c \gets \operatorname{clip}(q_1 q_2, \varepsilon, 1)$ \Comment{epistemic confidence}
        \State $\tau \gets \tau_0 / c$ if $c < \tau_{\text{thresh}}$ else $\tau_0$
        \State $p \gets \softmax(z / \tau)$
        \State $u_{\text{uniform}} \gets \mathbf{1} / |p|$
        \State $p_{\text{gated}} \gets c \cdot p + (1 - c) \cdot u_{\text{uniform}}$
        \State $u \gets 1 - c$ \Comment{epistemic uncertainty scalar}
        \State \Return $p_{\text{gated}}, u$
    \end{algorithmic}
\end{algorithm}

The gating interpolates between a confident softmax distribution and a maximally uncertain uniform distribution. Returning $p_{\text{gated}}$ and $u$ makes explicit that epistemic softmax outputs both a calibrated distribution and an uncertainty scalar.

\subsection{Properties}
Epistemic softmax reduces to standard softmax when $Q_1 = Q_2 = 1$, outputs uniform distributions when $Q_1 = Q_2 = 0$, remains differentiable, and exposes explicit uncertainty $u = 1 - Q_1 Q_2$.

\subsection{Comparison with Standard Softmax}

Table~\ref{tab:softmax_comparison} summarizes the key differences between standard softmax and epistemic softmax, highlighting how the latter addresses fundamental limitations of forced normalization.

\begin{table}[h]
\centering
\caption{Comparison between standard softmax and epistemic softmax.}
\label{tab:softmax_comparison}
\begin{tabular}{ll}
\toprule
\textbf{Standard Softmax} & \textbf{Epistemic Softmax} \\
\midrule
Inputs logits & Inputs logits + gates \\
Temperature fixed & Temperature adaptive \\
Outputs $p$ & Outputs $\tilde{p}$, $u$ \\
Forced confidence & Confidence modulated \\
No uncertainty signal & Explicit uncertainty \\
\bottomrule
\end{tabular}
\end{table}

\section{Fractal Architecture}

The Aletheion architecture applies epistemic softmax hierarchically across all transformer components, creating a fractal pattern of uncertainty quantification. Figure~\ref{fig:fractal_architecture} illustrates the flow of epistemic gates through attention mechanisms, head aggregation, and output generation.

\begin{figure}[h]
\centering
\small
\begin{verbatim}
Input Tokens
     |
[Embedding + Positional]
     |
┌───────────────────────────┐
│ Layer ℓ                    │
│ ┌───────────────────────┐ │
│ │ Attention Logits      │ │
│ │ epistemic_softmax     │◄┤ Q₁ (per-head)
│ └───────────────────────┘ │
│             │              │
│      Attention Output      │
│             │              │
│ ┌───────────────────────┐ │
│ │ Head Aggregation      │ │
│ │ Q₂ Consensus Gate     │◄┤ Cross-head
│ └───────────────────────┘ │
│             │              │
│     Residual + MLP         │
│             │              │
│   uncertainty_ℓ propagated │
└───────────────────────────┘
     |
    ... (repeat for L layers)
     |
[Final Hidden State + aggregated uncertainty]
     |
┌───────────────────────────┐
│ Output Logits             │
│ epistemic_softmax         │◄┤ Q₁ + Q₂ (global)
└───────────────────────────┘
     |
P(tokens), uncertainty_final
\end{verbatim}
\caption{\textbf{Fractal epistemic architecture.} Each layer applies epistemic softmax to attention weights (per-head $Q_1$ gates) and head aggregation ($Q_2$ consensus gate). Uncertainty propagates through layers and combines at the output, creating a multi-scale epistemic hierarchy.}
\label{fig:fractal_architecture}
\end{figure}

\subsection{Level 1: Output-Only}
Let $h_t$ denote decoder state, $z = W h_t$ the logits, and $c^{(\mathrm{out})}$ the context features (e.g., hidden state, attention summary). Epistemic softmax yields $(p_t, u_t) = \epsoftmax(z, c^{(\mathrm{out})})$. Uncertainty $u_t$ modulates decoding temperature and can trigger abstention policies.

\subsection{Level 2: Attention + Output}

Level 2 applies epistemic softmax to both attention mechanisms and output distributions.

\subsubsection*{Attention with Epistemic Gating}

For layer $l$ and head $h$, we first compute attention logits:
\begin{equation}
a^{(l,h)} = \frac{Q^{(l,h)} (K^{(l,h)})^\top}{\sqrt{d_k}} \in \mathbb{R}^{n \times n}
\end{equation}
where $Q^{(l,h)} = h^{(l-1)} W_Q^{(l,h)}$ and $K^{(l,h)} = h^{(l-1)} W_K^{(l,h)}$ are the projected query and key matrices.

We then apply epistemic softmax to obtain gated attention weights:
\begin{equation}
(p^{(l,h)}_{\text{att}}, u^{(l,h)}_{\text{att}}) = \texttt{EpSoftmax}(a^{(l,h)}, c^{(l,h)}_{\text{att}})
\end{equation}
where $c^{(l,h)}_{\text{att}} = Q^{(l,h)}_{[:,0,:]}$ is the context vector (we use the first query position as representative context, though any pooling strategy works).

The gated attention is applied to values:
\begin{equation}
o^{(l,h)} = p^{(l,h)}_{\text{att}} \cdot V^{(l,h)} \in \mathbb{R}^{n \times d_v}
\end{equation}
where $V^{(l,h)} = h^{(l-1)} W_V^{(l,h)}$.

\subsubsection*{Head Aggregation with Epistemic Gating}

After computing outputs from all $H$ heads, we aggregate them using a second epistemic gate. First, concatenate head outputs:
\begin{equation}
\text{head\_concat}^{(l)} = [o^{(l,1)} \, || \, o^{(l,2)} \, || \, \cdots \, || \, o^{(l,H)}] \in \mathbb{R}^{n \times d}
\end{equation}

To determine how to weight each head, we compute aggregation logits via a learned MLP:
\begin{equation}
w^{(l)} = \text{MLP}_{\text{agg}}(\text{mean}(\text{head\_concat}^{(l)})) \in \mathbb{R}^H
\end{equation}
where $\text{MLP}_{\text{agg}}$ is a small feedforward network that outputs $H$ scalar logits.

We construct the context for the head aggregation gate:
\begin{equation}
c^{(l)}_{\text{head}} = \text{mean}_{\text{seq}}(\text{head\_concat}^{(l)}) \in \mathbb{R}^d
\end{equation}
(mean pooling over the sequence dimension).

Apply epistemic softmax to obtain head mixing weights:
\begin{equation}
(p^{(l)}_{\text{head}}, u^{(l)}_{\text{head}}) = \texttt{EpSoftmax}(w^{(l)}, c^{(l)}_{\text{head}})
\end{equation}
where $p^{(l)}_{\text{head}} \in \mathbb{R}^H$ is a probability distribution over heads.

The final layer output is the weighted combination:
\begin{equation}
h^{(l)}_{\text{attn}} = \sum_{h=1}^{H} p^{(l)}_{\text{head},h} \cdot o^{(l,h)} \in \mathbb{R}^{n \times d}
\end{equation}

\subsubsection*{Layer Uncertainty Aggregation}

The combined uncertainty for layer $l$ aggregates uncertainties from all heads and the head mixing gate:
\begin{equation}
u^{(l)} = \max\left(\max_{h \in [H]} u^{(l,h)}_{\text{att}}, \, u^{(l)}_{\text{head}}\right)
\end{equation}

This conservative aggregation ensures that if \emph{any} head or the aggregation is uncertain, the layer reflects that uncertainty.

\subsubsection*{Complete Layer Forward Pass}

The complete forward pass for layer $l$ is:
\begin{align}
h^{(l)}_{\text{attn}} &= \text{LayerNorm}\left(h^{(l-1)} + \text{MultiHeadAttn}_{\text{epistemic}}(h^{(l-1)})\right) \\
h^{(l)} &= \text{LayerNorm}\left(h^{(l)}_{\text{attn}} + \text{FFN}(h^{(l)}_{\text{attn}})\right)
\end{align}
where $\text{MultiHeadAttn}_{\text{epistemic}}$ incorporates all the epistemic gating described above.

\subsection{Level 3: Full Fractal}
Level 3 replaces every softmax invocation---mixture-of-experts routers, adaptive span controllers, key-value selection---with epistemic softmax. Each module exports an uncertainty scalar; the layer exposes $(y^{(l)}, u^{(l)})$. Uncertainty composition follows a monotone aggregation function $f$:
\begin{equation}
    u_{\mathrm{final}} = f\bigl(u_{\mathrm{att}}^{(1)}, \dots, u_{\mathrm{att}}^{(L)}, u_{\mathrm{head}}^{(1)}, \dots, u_{\mathrm{head}}^{(L)}, u_{\mathrm{out}}\bigr).
\end{equation}
Choices include $\max$ (conservative), mean (smooth), or a learned aggregator trained to predict downstream errors.

\subsection{Fractal Pseudocode}
\begin{algorithm}[H]
\caption{Fractal Epistemic Transformer (Forward Pass)}
\label{alg:fractal}
\begin{algorithmic}[1]
\Require Token sequence $x = (x_1, \ldots, x_n)$
\Ensure Probability distribution $p_{\text{out}}$, uncertainty $u_{\text{final}}$
\State $h^{(0)} \gets \text{Embed}(x) + \text{PositionalEncoding}(x)$
\State
\For{$l = 1$ to $L$}
    \State \textcolor{blue}{// Multi-head attention with epistemic gating}
    \For{$h = 1$ to $H$}
        \State $Q^{(l,h)} \gets h^{(l-1)} W_Q^{(l,h)}$
        \State $K^{(l,h)} \gets h^{(l-1)} W_K^{(l,h)}$
        \State $V^{(l,h)} \gets h^{(l-1)} W_V^{(l,h)}$
        \State
        \State \textcolor{blue}{// Compute attention logits}
        \State $a^{(l,h)} \gets (Q^{(l,h)} (K^{(l,h)})^\top) / \sqrt{d_k}$
        \State
        \State \textcolor{blue}{// Apply epistemic softmax to attention}
        \State $c^{(l,h)}_{\text{att}} \gets Q^{(l,h)}_{[:,0,:]}$ \hfill $\triangleright$ use first query as context
        \State $(p^{(l,h)}_{\text{att}}, u^{(l,h)}_{\text{att}}) \gets \texttt{EpSoftmax}(a^{(l,h)}, c^{(l,h)}_{\text{att}})$
        \State
        \State \textcolor{blue}{// Apply gated attention to values}
        \State $o^{(l,h)} \gets p^{(l,h)}_{\text{att}} \cdot V^{(l,h)}$
    \EndFor
    \State
    \State \textcolor{blue}{// Aggregate heads with epistemic gating}
    \State $\text{head\_concat} \gets [o^{(l,1)} \, || \, \cdots \, || \, o^{(l,H)}]$
    \State $w^{(l)} \gets \text{MLP}_{\text{agg}}(\text{mean}(\text{head\_concat}))$ \hfill $\triangleright$ produces $H$ logits
    \State $c^{(l)}_{\text{head}} \gets \text{mean}_{\text{seq}}(\text{head\_concat})$ \hfill $\triangleright$ aggregated context
    \State $(p^{(l)}_{\text{head}}, u^{(l)}_{\text{head}}) \gets \texttt{EpSoftmax}(w^{(l)}, c^{(l)}_{\text{head}})$
    \State
    \State \textcolor{blue}{// Weighted head combination}
    \State $h^{(l)}_{\text{attn}} \gets \sum_{h=1}^{H} p^{(l)}_{\text{head},h} \cdot o^{(l,h)}$
    \State
    \State \textcolor{blue}{// Apply residual + FFN}
    \State $h^{(l)}_{\text{attn}} \gets \text{LayerNorm}(h^{(l-1)} + h^{(l)}_{\text{attn}})$
    \State $h^{(l)} \gets \text{LayerNorm}(h^{(l)}_{\text{attn}} + \text{FFN}(h^{(l)}_{\text{attn}}))$
    \State
    \State \textcolor{blue}{// Layer uncertainty}
    \State $u^{(l)} \gets \max(\max_h u^{(l,h)}_{\text{att}}, u^{(l)}_{\text{head}})$
\EndFor
\State
\State \textcolor{blue}{// Output distribution with epistemic gating}
\State $\text{logits} \gets h^{(L)} W_{\text{vocab}}$
\State $c_{\text{out}} \gets \text{mean}_{\text{seq}}(h^{(L)})$
\State $(p_{\text{out}}, u_{\text{out}}) \gets \texttt{EpSoftmax}(\text{logits}, c_{\text{out}})$
\State
\State \textcolor{blue}{// Final uncertainty aggregation}
\State $u_{\text{final}} \gets \max(u^{(1)}, \ldots, u^{(L)}, u_{\text{out}})$
\State
\Return $p_{\text{out}}, u_{\text{final}}$
\end{algorithmic}
\end{algorithm}

\subsection{Uncertainty Propagation}
For a transformer with $L$ layers, conservative deployment adopts
\begin{equation}
    u_{\mathrm{final}} = \max\bigl(\max_{l} u_{\mathrm{att}}^{(l)}, u_{\mathrm{out}}\bigr).
\end{equation}
Learned aggregators can be implemented as small monotone networks that take concatenated uncertainties and output a calibrated scalar.

Figure~\ref{fig:uncertainty_flow} illustrates how layer-wise uncertainties aggregate into a final epistemic signal that can drive confidence-aware decoding and exploration strategies.

\begin{figure}[h]
\centering
\small
\begin{verbatim}
Layer 1 Uncertainty (u₁) ──┐
                           ▼
Layer 2 Uncertainty (u₂) ──┐    Aggregate g(u)
                           │         │
⋮                          │         ▼
Layer N Uncertainty (uₙ) ──┘   u_final → Exploration Controller
                                       │
                           Confidence-aware Decoding
                                       │
                             Response + uncertainty_final
\end{verbatim}
\caption{\textbf{Uncertainty flow through the epistemic architecture.} Layer-wise uncertainties are aggregated (e.g., via $\max$ or learned function $g$) into a final uncertainty scalar that drives exploration and confidence-aware decoding strategies.}
\label{fig:uncertainty_flow}
\end{figure}

\section{Training with VARO}
\subsection{Supervisory Signal $u^*$}
Training requires a target uncertainty $u^*$:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Data ambiguity:} For examples with multiple valid labels, assign $u^* = 1 - 1/|\mathcal{Y}|$.
    \item \textbf{Head variance:} Estimate $u^*$ using variance of attention head outputs: $u^* = \sigma^2(\{z_h\}) / (\sigma^2(\{z_h\}) + 1)$.
    \item \textbf{Distributional distance:} Detect out-of-distribution tokens via density models or embedding distances, mapping high distances to high $u^*$.
    \item \textbf{Self-consistency probes:} Monte Carlo decoding disagreement supplies additional targets during fine-tuning.
\end{enumerate}

\subsubsection{Practical Implementation Strategy}

The choice of $u^*$ depends on the training phase and available supervision:

\paragraph{Phase 0-1: Pre-training (no labeled uncertainty)}
Use \textbf{Method 2 (Head Variance)}:
\begin{equation}
u^* = \frac{\sigma^2(\{z_h\})}{\sigma^2(\{z_h\}) + 1}
\end{equation}
where $z_h$ are logits from different attention heads. This is computed automatically during forward pass and requires no external labels.

\textit{Implementation:}
\begin{verbatim}
heads_logits = [head_1.logits, ..., head_H.logits]  # [H, B, T, V]
variance = torch.var(heads_logits, dim=0)            # [B, T, V]
u_star = variance / (variance + 1)                   # normalize to [0,1]
\end{verbatim}

\paragraph{Phase 2: Fine-tuning with labeled data}
Use \textbf{Method 1 (Data Ambiguity)}:

For examples with multiple valid labels $Y = \{y_1, \ldots, y_k\}$:
\begin{equation}
u^* = 1 - \frac{1}{|Y|}
\end{equation}

\textit{Example:} Question "What is the capital of the Netherlands?"
\begin{itemize}
    \item If dataset has both "Amsterdam" (official capital) and "The Hague" (seat of government):
    \item $Y = \{\text{Amsterdam}, \text{The Hague}\}$, thus $|Y| = 2$
    \item Therefore $u^* = 1 - 1/2 = 0.5$ (high ambiguity)
\end{itemize}

\textit{Implementation:}
\begin{verbatim}
if len(valid_labels) > 1:
    u_star = 1.0 - 1.0/len(valid_labels)
else:
    u_star = 0.0  # unambiguous example
\end{verbatim}

\paragraph{Phase 3: Out-of-Distribution Detection}
Use \textbf{Method 3 (Distributional Distance)}:
\begin{equation}
u^* = \min\left(1, \frac{d(x, X_{\text{train}})}{d_{\max}}\right)
\end{equation}
where $d(x, X_{\text{train}})$ is the distance from input $x$ to the nearest training example.

\textit{Implementation using embedding distance:}
\begin{verbatim}
emb_x = encoder(x)                           # current input embedding
emb_train = encoder(X_train_sample)          # sample from training set
distances = torch.cdist(emb_x, emb_train)    # pairwise distances
min_dist = torch.min(distances)
u_star = torch.clamp(min_dist / d_max, 0, 1)
\end{verbatim}

\paragraph{Phase 4: Post-training validation}
Use \textbf{Method 4 (Self-Consistency)}:

Generate $K$ responses, measure disagreement:
\begin{equation}
u^* = 1 - \text{(agreement rate)}
\end{equation}
where $\text{agreement rate} = \frac{\text{count of most common response}}{K}$.

\textit{Implementation:}
\begin{verbatim}
responses = [model.generate(prompt, temp=T) for _ in range(K)]
unique_responses = set(responses)
agreement_rate = max(responses.count(r) for r in unique_responses) / K
u_star = 1.0 - agreement_rate
\end{verbatim}

\paragraph{Combining methods}
In practice, use a weighted combination:
\begin{equation}
u^* = w_1 \cdot u^*_{\text{variance}} + w_2 \cdot u^*_{\text{ambiguity}} + w_3 \cdot u^*_{\text{distance}}
\end{equation}
where weights $\{w_i\}$ depend on available supervision signals and sum to 1.

\subsection{Pyramidal VARO Loss}

The pyramidal architecture requires a multi-component loss that calibrates each epistemic gate independently while maintaining base stability and height consistency. The total loss is:
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{CE}} + \lambda_{\text{base}} \mathcal{L}_{\text{base}} + \lambda_{Q_1} \mathcal{L}_{Q_1} + \lambda_{Q_2} \mathcal{L}_{Q_2} + \lambda_{\text{fractal}} \mathcal{L}_{\text{fractal}} + \lambda_{\text{height}} \mathcal{L}_{\text{height}}
\end{equation}

\paragraph{Cross-Entropy Loss:}
\begin{equation}
\mathcal{L}_{\text{CE}} = -\log p_{\text{gated}}(y^* \mid x)
\end{equation}
Standard task loss for next-token prediction.

\paragraph{Base Stability Loss:}
\begin{equation}
\mathcal{L}_{\text{base}} = \text{Var}(\mathbf{b}) = \frac{1}{4}\sum_{i=1}^{4}(w_i - 0.25)^2
\end{equation}
Penalizes imbalance in the base simplex. Encourages equal weighting of Memory, Pain, Choice, Exploration unless task-specific adaptation is required.

\paragraph{$Q_1$ Calibration Loss:}
\begin{equation}
\mathcal{L}_{Q_1} = \|Q_1 - Q_1^*\|_2^2, \quad \text{where } Q_1^* = 1 - p(y^* \mid x)
\end{equation}
Aligns $Q_1$ with aleatoric uncertainty: high when correct token has low probability.

\paragraph{$Q_2$ Calibration Loss:}
\begin{equation}
\mathcal{L}_{Q_2} = \|Q_2 - Q_2^*\|_2^2, \quad \text{where } Q_2^* = \frac{1}{2}\left[(1 - \mathbb{1}[\arg\max p = y^*]) + \frac{H(p)}{\log V}\right]
\end{equation}
Aligns $Q_2$ with epistemic uncertainty: high when model is both wrong and uncertain (high entropy).

\paragraph{Fractal Regularization Loss:}
\begin{equation}
\mathcal{L}_{\text{fractal}} = \sigma_{Q_1}^2 + \sigma_{Q_2}^2
\end{equation}
Penalizes excessive fractal variance to prevent meta-uncertainty from exploding. Encourages confident uncertainty estimates.

\paragraph{Height Consistency Loss:}
\begin{equation}
\mathcal{L}_{\text{height}} = \left\|h - \sigma\left(W_h \cdot \begin{bmatrix} 1-Q_1 \\ 1-Q_2 \\ s_{\text{base}} \end{bmatrix}\right)\right\|_2^2
\end{equation}
Ensures height is derived from $Q_1$, $Q_2$, and base stability, preventing free drift.

\paragraph{Gradient Flow:}
Gradients propagate through all gates simultaneously:
\begin{align}
\frac{\partial \mathcal{L}}{\partial Q_1} &= \lambda_{Q_1} \cdot 2(Q_1 - Q_1^*) + \lambda_{\text{height}} \cdot \frac{\partial \mathcal{L}_{\text{height}}}{\partial Q_1} \\
\frac{\partial \mathcal{L}}{\partial Q_2} &= \lambda_{Q_2} \cdot 2(Q_2 - Q_2^*) + \lambda_{\text{height}} \cdot \frac{\partial \mathcal{L}_{\text{height}}}{\partial Q_2} \\
\frac{\partial \mathcal{L}}{\partial \sigma_{Q_i}} &= \lambda_{\text{fractal}} \cdot 2\sigma_{Q_i}, \quad i \in \{1, 2\}
\end{align}

The multi-component loss prevents gate collapse by providing independent supervision for $Q_1$ and $Q_2$. Unlike the tetrahedral formulation where $u = 1 - Q_1 Q_2$ collapsed both gates toward 1, the pyramidal loss disentangles aleatoric and epistemic modes.

\paragraph{Recommended Hyperparameters:}
Based on empirical trials and collapse prevention analysis:
\begin{itemize}[leftmargin=*]
    \item $\lambda_{\text{base}} = 0.01$: Light regularization of base balance
    \item $\lambda_{Q_1} = 0.015$: Moderate aleatoric calibration
    \item $\lambda_{Q_2} = 0.020$: Stronger epistemic calibration (epistemic more critical)
    \item $\lambda_{\text{fractal}} = 0.005$: Light meta-uncertainty control
    \item $\lambda_{\text{height}} = 0.02$: Strong height derivation enforcement
\end{itemize}

These values ensure the CE loss dominates (implicitly weighted at 1.0) while epistemic components provide sufficient gradient signal to prevent collapse.

\subsection{Training Phases}
\begin{enumerate}[leftmargin=*]
    \item \textbf{Phase 0: Baseline pretraining.} Train a standard transformer with cross-entropy until convergence.
    \item \textbf{Phase 1: Gate warm-start.} Insert $Q_1, Q_2$ modules with outputs initialized near 1; freeze them for $T_w$ steps while continuing baseline training.
    \item \textbf{Phase 2: VARO activation.} Unfreeze gates, enable VARO with schedule $\lambda_t$, and introduce uncertainty targets $u^*$.
    \item \textbf{Phase 3: Epistemic decoding.} Use $u$ to control temperature, abstention, retrieval triggers, and self-consistency sampling.
\end{enumerate}

\subsection{Optimization Considerations}
Gradient stability benefits from clipping $u$ within $[\varepsilon, 1-\varepsilon]$. Gate architectures can share parameters across layers to reduce overhead, and entropy regularizers discourage gate collapse (always-on or always-off behavior).

\section{Adaptive Epistemic Dynamics: Emergent Metalearning}

During Q1Q2 training, we observed sophisticated adaptive behavior where the model actively explores the epistemic parameter space to optimize calibration.



\begin{figure}[t]
\centering
\begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/baseline_training_curves.png}
    \caption{Baseline Transformer}
    \label{fig:baseline_training}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/pyramidal_training_curves.png}
    \caption{Pyramidal Architecture (without Q1/Q2 gates)}
    \label{fig:pyramidal_training}
\end{subfigure}
\caption{\textbf{Comparative training dynamics: Baseline vs.\ Pyramidal architectures over 60{,}000 steps.} 
\textbf{Left panel (Baseline):} Standard transformer (GPT-2) showing \textcolor{blue}{training loss} (solid blue) and \textcolor{orange}{evaluation loss} (dotted orange) decreasing monotonically, with \textcolor{purple}{perplexity} (purple) stabilizing after 20k steps. The baseline exhibits conventional convergence but progressively \textbf{loses calibration} as Expected Calibration Error (ECE) rises and Brier Score stagnates, indicating \textbf{overconfidence without epistemic awareness}. 
\textbf{Right panel (Pyramidal):} Ungated pyramidal architecture showing similar loss convergence but with a \textbf{geometric substrate for epistemic quantification}. The four base forces (Memory, Pain, Choice, Exploration) maintain near-balanced dynamics and high base stability ($>0.9$). However, without active Q1/Q2 gates, the model exhibits the \textbf{Skynet phenomenon}---a drift toward the Apex (Height $\rightarrow 1.0$) accompanied by deteriorating calibration (ECE $\uparrow$), reflecting an emergent self-belief and reduced sensitivity to unknowns. 
The pyramidal design thus enables explicit monitoring of epistemic collapse: even when loss converges, the geometry reveals \textbf{apex delusion} versus \textbf{calibrated ascent}.}

\label{fig:baseline_pyramidal_comparison}
\end{figure}

\subsection{Exploration Cycles}

Between steps 2100--2750, the model exhibited cyclic exploration:

\paragraph{Phase 1 (Step 2400): Q1/Q2 spike to 0.40/0.45}
\begin{itemize}[leftmargin=*]
    \item Testing high uncertainty configuration
    \item ECE degraded to 0.086
    \item System rejected this configuration
\end{itemize}

\paragraph{Phase 2 (Step 2700): Q1/Q2 dropped to 0.11/0.13}
\begin{itemize}[leftmargin=*]
    \item Testing low uncertainty (near-saturation)
    \item Collapse warnings triggered
    \item System rejected this configuration
\end{itemize}

\paragraph{Phase 3 (Step 2750): Q1/Q2 stabilized at 0.42/0.47}
\begin{itemize}[leftmargin=*]
    \item Found optimal mid-range
    \item ECE improved to 0.074
    \item Q1/Q2 distinction restored
\end{itemize}

\subsection{Dataset-Aware Convergence}

The ``Q1/Q2 not distinct'' warning (gap $< 0.05$) emerged not from architectural failure, but from the model discovering dataset properties:

For deterministic, well-understood datasets:
\begin{itemize}[leftmargin=*]
    \item Low aleatoric uncertainty ($Q_1 \approx 0.15$--$0.20$)
    \item Low epistemic uncertainty ($Q_2 \approx 0.18$--$0.22$)
    \item Small gap is correct, not problematic
\end{itemize}

This adaptive behavior validates architectural flexibility: Q1Q2 gates maintain separation when needed, but allow convergence when data structure permits it.

Critically, validation sets maintained Q1/Q2 distinction even when training showed temporary convergence (train $Q_1=0.112$, $Q_2=0.130$ at step 2700; val $Q_1=0.468$, $Q_2=0.474$ at same step), confirming the behavior represents active exploration rather than architectural failure.

\subsection{Implications}

This emergent metalearning demonstrates:
\begin{enumerate}[leftmargin=*]
    \item The architecture adapts to data structure rather than imposing rigid separation
    \item Collapse warnings signal exploration phases, not failure modes
    \item The model self-corrects through gradient dynamics
    \item Q1Q2 separation is maintained when epistemically meaningful
\end{enumerate}

\subsection{Epistemic Saturation}

At step~41{,}350, the system reaches a state of \textbf{epistemic saturation},
where both uncertainty gates output minimal values ($Q_1=0.017$, $Q_2=0.072$)
while \textbf{Height = 1.000}.
This configuration indicates that within its learned distribution the model
has exhausted its epistemic variability---a condition analogous to an internal
belief of completeness, though still bounded to the optimization domain.

At step~41{,}850 the model reaches a frozen apex state (Height $=1.000$,
Stability $=1.000$) with vanishing multi-scale variability (Fractal $=0$).
Epistemic gates are effectively silenced ($Q_1\!=\!0.025$, $Q_2\!=\!0.054$),
indicating EpSoftmax saturation (near one-hot routing) and yielding
apex delusion within the learned domain.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/pyramidal_q1q2_curves.png}
\caption{\textbf{Q1/Q2 trajectories over training 60000 steps.} The figure shows three distinct phases: initial high-uncertainty exploration (step 2400), low-uncertainty collapse testing (step 2700), and stabilization at optimal mid-range values (step 2750+). Validation metrics (shown in dashed lines) maintain separation throughout, confirming that training dynamics represent exploration rather than architectural failure.}
\label{fig:pyramidal_q1q2_curves}
\end{figure}

\section{Theoretical Analysis}
\subsection{Monotone Uncertainty Propagation}
\begin{theorem}[Uncertainty Propagation]
Let $h^{(l+1)} = f_l(h^{(l)}, p_{\mathrm{gated}}^{(l)})$ denote the representation update at layer $l$ and $u^{(l)}$ the uncertainty emitted by that layer. Suppose aggregation uses a monotone non-decreasing function $f$. Then the final uncertainty satisfies
\begin{equation}
    u_{\mathrm{final}} \geq \max_{0 \leq l \leq L} u^{(l)}.
\end{equation}
\end{theorem}
\begin{proof}[Proof of Theorem 1]
We prove that $u_{\text{final}} \geq \max_{0 \leq l \leq L} u^{(l)}$ for monotone aggregation function $f$.

\textbf{Step 1:} By definition of the aggregation function:
\begin{equation}
u_{\text{final}} = f(u^{(0)}, u^{(1)}, \ldots, u^{(L)})
\end{equation}

\textbf{Step 2:} Let $u_{\max} = \max_{0 \leq l \leq L} u^{(l)}$ and let $l^* \in \{0, \ldots, L\}$ be the layer achieving this maximum:
\begin{equation}
u^{(l^*)} = u_{\max}
\end{equation}

\textbf{Step 3:} Consider the input vector to $f$ where we set all entries except $l^*$ to zero:
\begin{equation}
v_{\min} = (0, 0, \ldots, \underbrace{u_{\max}}_{l^*}, \ldots, 0) \in [0,1]^{L+1}
\end{equation}

\textbf{Step 4:} Since $f$ is monotone non-decreasing in each argument and $u^{(l)} \geq 0$ for all $l$:
\begin{equation}
f(u^{(0)}, \ldots, u^{(L)}) \geq f(v_{\min}) = f(0, \ldots, u_{\max}, \ldots, 0)
\end{equation}

\textbf{Step 5:} For specific aggregation functions:
\begin{itemize}
    \item \textbf{Max aggregator:} $f = \max$
    \begin{equation}
    f(0, \ldots, u_{\max}, \ldots, 0) = u_{\max}
    \end{equation}

    \item \textbf{Mean aggregator:} $f = \text{mean}$
    \begin{equation}
    f(u^{(0)}, \ldots, u^{(L)}) = \frac{1}{L+1} \sum_{l=0}^{L} u^{(l)} \geq \frac{u_{\max}}{L+1}
    \end{equation}
    However, this is a weaker bound. To achieve $u_{\text{final}} \geq u_{\max}$, we require $f$ to satisfy:
    \begin{equation}
    f(\ldots, u_{\max}, \ldots) \geq u_{\max}
    \end{equation}
    which holds for $f = \max$ and learned monotone aggregators with appropriate initialization.

    \item \textbf{Learned aggregator:} Train $f$ to satisfy $f(v) \geq \max_i v_i$ via architectural constraints (e.g., max-pooling layer followed by learned transformation).
\end{itemize}

\textbf{Step 6:} Therefore, for conservative aggregation ($f = \max$):
\begin{equation}
u_{\text{final}} = \max(u^{(0)}, \ldots, u^{(L)}) = \max_{0 \leq l \leq L} u^{(l)}
\end{equation}

\textbf{Step 7:} Residual connections preserve this property because epistemic gates multiply probability distributions rather than subtract scalars. If layer $l$ has high uncertainty $u^{(l)} \approx 1$, the gated distribution $p^{(l)}_{\text{gated}} \approx \text{uniform}$. Subsequent layers cannot "undo" this uncertainty without evidence.

\textbf{Step 8:} Thus, $u_{\text{final}} \geq \max_{l} u^{(l)}$ as required.
\end{proof}

\begin{corollary}
If any layer emits high uncertainty ($u^{(l)}$ close to 1), the final output uncertainty cannot collapse to zero unless all subsequent layers emit perfect certainty ($u = 0$), which is unlikely under VARO training that penalizes miscalibrated confidence.
\end{corollary}

\subsection{Calibration Under VARO}
\begin{theorem}[Calibration Under VARO]
Consider training an epistemic softmax model with stochastic gradient descent on the loss:
\begin{equation}
\mathcal{L}(\theta) = \mathcal{L}_{\text{CE}}(p_{\text{gated}}(\theta), y) + \lambda \|u(\theta) - u^*\|_2^2
\end{equation}
where $\theta$ are model parameters, $p_{\text{gated}}$ is the gated distribution from Algorithm 1, $u$ is predicted uncertainty, and $u^*$ is target uncertainty.

\textbf{Assumptions:}
\begin{enumerate}[label=(A\arabic*)]
    \item \textbf{$L$-smoothness:} The loss $\mathcal{L}$ is $L$-smooth, i.e., for all $\theta, \theta'$:
    \begin{equation}
    \|\nabla \mathcal{L}(\theta) - \nabla \mathcal{L}(\theta')\| \leq L \|\theta - \theta'\|
    \end{equation}

    \item \textbf{Unbiased uncertainty targets:} The target uncertainty $u^*$ satisfies:
    \begin{equation}
    \mathbb{E}[u^* \mid x] = u_{\text{true}}(x)
    \end{equation}
    where $u_{\text{true}}(x)$ is the true epistemic uncertainty for input $x$.

    \item \textbf{Bounded gradient variance:} For stochastic gradients $g_t$:
    \begin{equation}
    \mathbb{E}[\|g_t - \nabla \mathcal{L}(\theta_t)\|^2] \leq \sigma^2
    \end{equation}

    \item \textbf{Robbins-Monro learning rate:} The learning rate $\eta_t$ satisfies:
    \begin{equation}
    \sum_{t=1}^{\infty} \eta_t = \infty \quad \text{and} \quad \sum_{t=1}^{\infty} \eta_t^2 < \infty
    \end{equation}
\end{enumerate}

Under assumptions (A1)-(A4), the expected calibration error (ECE) decreases:
\begin{equation}
\mathbb{E}[\text{ECE}_{t+1}] \leq \mathbb{E}[\text{ECE}_t] - \eta_t \lambda c_1 + \eta_t^2 c_2
\end{equation}
where:
\begin{align}
c_1 &= 2 \mathbb{E}[\|\nabla_u \text{ECE}\|^2] \quad \text{(gradient of ECE w.r.t. uncertainty)} \\
c_2 &= L\sigma^2 + \frac{L^2}{2} \quad \text{(smoothness and variance terms)}
\end{align}

\textbf{Corollary:} Choosing $\eta_t = 1/t$ yields:
\begin{equation}
\mathbb{E}[\text{ECE}_T] \leq \mathbb{E}[\text{ECE}_0] - \lambda c_1 \log(T) + O(1)
\end{equation}
Thus ECE decreases logarithmically with training steps $T$.
\end{theorem}

\begin{proof}[Proof sketch]
\textbf{Step 1:} By $L$-smoothness (A1) and the SGD update rule $\theta_{t+1} = \theta_t - \eta_t g_t$:
\begin{equation}
\mathbb{E}[\mathcal{L}(\theta_{t+1})] \leq \mathbb{E}[\mathcal{L}(\theta_t)] - \eta_t \mathbb{E}[\|\nabla \mathcal{L}(\theta_t)\|^2] + \frac{L\eta_t^2}{2} \mathbb{E}[\|g_t\|^2]
\end{equation}

\textbf{Step 2:} The VARO term $\lambda \|u - u^*\|^2$ provides gradient:
\begin{equation}
\nabla_{\theta} \left(\lambda \|u - u^*\|^2\right) = 2\lambda (u - u^*) \nabla_{\theta} u
\end{equation}

\textbf{Step 3:} By assumption (A2), $\mathbb{E}[u - u^*]$ measures calibration error, which correlates with ECE. Specifically, ECE is defined as:
\begin{equation}
\text{ECE} = \mathbb{E}_{B \in \text{bins}} \left| \frac{1}{|B|} \sum_{i \in B} \mathbb{1}[y_i = \hat{y}_i] - \bar{c}_B \right|
\end{equation}
where $\bar{c}_B$ is the average confidence in bin $B$.

The VARO loss directly optimizes $\|u - u^*\|^2$, and under proper calibration ($u^* = 1 - \text{confidence}$), minimizing this term reduces the gap between confidence and accuracy, thereby reducing ECE.

\textbf{Step 4:} Substituting (A3) and applying standard SGD convergence analysis (see \cite{bottou2018optimization}) with (A4) yields the stated bound.

Full proof requires technical analysis of ECE geometry \cite{guo2017calibration} and is deferred to supplementary material.
\end{proof}

\subsection{Computational Complexity}

\subsubsection*{Standard Transformer Cost}
For $L$ layers, sequence length $n$, and hidden dimension $d$:
\begin{align}
\text{Attention:} \quad & O(L \cdot n^2 \cdot d) \quad \text{(all layers)} \\
\text{Feed-forward:} \quad & O(L \cdot n \cdot d^2) \quad \text{(all layers)} \\
\text{Total:} \quad & O(L \cdot n^2 \cdot d + L \cdot n \cdot d^2)
\end{align}

\subsubsection*{Epistemic Softmax Overhead}
Each gate $Q_i$ is an MLP with hidden size $k$:
\begin{align}
\text{Forward:} \quad & O(d \cdot k + k \cdot 1) = O(d \cdot k) \quad \text{per invocation} \\
\text{Memory:} \quad & O(d \cdot k) \quad \text{parameters per gate}
\end{align}

\subsubsection*{Level-by-Level Analysis}

\paragraph{Level 1 (Output-only)}
\begin{itemize}
    \item \textbf{Gates:} 1 $Q_1$ gate + 1 $Q_2$ gate at output layer
    \item \textbf{Operations:} $2 \times O(d \cdot k)$ per token $= O(n \cdot d \cdot k)$
    \item \textbf{Overhead:}
    \begin{equation}
    \frac{O(n \cdot d \cdot k)}{O(L \cdot n^2 \cdot d)} = \frac{k}{L \cdot n}
    \end{equation}
    \item \textbf{Numerical estimate:} For $k = d/4$, $L = 12$, $n = 512$:
    \begin{equation}
    \text{Overhead} \approx \frac{d/4}{12 \times 512} \approx 0.04\% \quad \text{(negligible)}
    \end{equation}
\end{itemize}

\paragraph{Level 2 (Attention + Output)}
\begin{itemize}
    \item \textbf{Gates per layer:}
    \begin{itemize}
        \item $H$ attention heads $\times$ 1 $Q_1$ gate each $= H \times O(d \cdot k)$
        \item 1 $Q_2$ gate for head aggregation $= O(d \cdot k)$
        \item Total per layer: $O(H \cdot d \cdot k)$
    \end{itemize}
    \item \textbf{Gates across $L$ layers:} $L \times O(H \cdot n \cdot d \cdot k)$
    \item \textbf{Plus output gates:} $O(n \cdot d \cdot k)$
    \item \textbf{Total overhead:}
    \begin{equation}
    \frac{O(L \cdot H \cdot n \cdot d \cdot k)}{O(L \cdot n^2 \cdot d)} = \frac{H \cdot k}{n}
    \end{equation}
    \item \textbf{Numerical estimate:} For $H = 8$, $k = d/4$, $n = 512$:
    \begin{equation}
    \text{Overhead} = \frac{8 \cdot (d/4)}{512} = \frac{2d}{512} \approx 2\% \quad \text{(for $d = 512$)}
    \end{equation}
    \item \textbf{Range:} 2-3\% depending on $d/n$ ratio
\end{itemize}

\paragraph{Level 3 (Full Fractal)}
\begin{itemize}
    \item \textbf{Additional gates:} MoE routers, adaptive attention mechanisms, etc.
    \item \textbf{Estimate:} Approximately $1.5\times$ Level 2 overhead
    \item \textbf{Total overhead:} $\approx$ 4-5\%
\end{itemize}

\subsubsection*{Parameter Overhead}

\paragraph{Without parameter sharing:}
\begin{itemize}
    \item $Q_1$ gate: $d \times k + k \times 1 \approx d \cdot k$ parameters
    \item $Q_2$ gate: $d \times k$ parameters
    \item \textbf{Level 1:} 2 gates $= 2dk$
    \begin{itemize}
        \item For $k = d/4$: $2d \cdot (d/4) = d^2/2$ parameters
        \item Baseline has $\approx 12d^2$ (for $L=12$ layers $\times$ projection matrices)
        \item Overhead: $(d^2/2)/(12d^2) \approx 4\%$ parameters
    \end{itemize}
    \item \textbf{Level 2:} $2L(H+1)$ gates
    \begin{itemize}
        \item For $L=12$, $H=8$: $2 \cdot 12 \cdot 9 = 216$ gates
        \item Parameters: $216 \cdot dk = 54d^2$ (for $k=d/4$)
        \item Overhead: $54d^2 / (12d^2 \cdot L) \approx 38\%$ parameters (significant!)
    \end{itemize}
\end{itemize}

\paragraph{With parameter sharing (recommended):}
\begin{itemize}
    \item Share $Q_1$ weights across all heads
    \item Share $Q_2$ weights across all layers
    \item \textbf{Level 2 parameters:} Just 2 gates $= 2dk$
    \item \textbf{Overhead:} $< 5\%$ even for Level 3
\end{itemize}

\subsubsection*{Memory-Computation Tradeoff}
\begin{itemize}
    \item \textbf{Without sharing:} Higher memory, same compute per forward pass
    \item \textbf{With sharing:} Lower memory, same compute per forward pass
    \item \textbf{Recommendation:} Share $Q_1$ across heads within a layer; unique $Q_2$ per layer to capture layer-specific consensus patterns
\end{itemize}

\subsubsection*{Empirical Measurement}
To be added after implementation:
\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Latency (ms/token)} & \textbf{Memory (GB)} & \textbf{Overhead} \\
\midrule
Baseline       & $X$ & $Y$ & --- \\
Level 1        & $X + \delta_1$ & $Y$ & $+0.5\%$ \\
Level 2        & $X + \delta_2$ & $Y + \epsilon$ & $+2.5\%$ \\
Level 3        & $X + \delta_3$ & $Y + \epsilon$ & $+4.5\%$ \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Robustness to Gate Collapse}
Gate collapse occurs when $Q_1$ or $Q_2$ saturate at 0 or 1. Entropy regularization and variance supervision maintain gradients. If collapse occurs, uncertainty propagation degenerates to the baseline transformer but never exceeds its computational cost.

\subsection{Pyramidal Stability Theorems}

We now formalize three theorems specific to the pyramidal architecture that establish its superiority over the tetrahedral formulation.

\begin{theorem}[Apex Attractor Property]
\label{thm:apex_attractor}
Let $\mathbf{s}(t) = (1-h(t)) \cdot \mathbf{b}(t) + h(t) \cdot \mathbf{apex}$ be the pyramidal state at training step $t$, where $h(t)$ is derived via Equation~3.2. Under the pyramidal VARO loss (Section~6.2) with $\lambda_{\text{height}} > 0$ and assuming convergence of $Q_1 \to Q_1^*$, $Q_2 \to Q_2^*$, the height coordinate satisfies:
\begin{equation}
\lim_{t \to \infty} \mathbb{E}[h(t)] = \sigma\left(W_h \cdot \begin{bmatrix} 1-\mathbb{E}[Q_1^*] \\ 1-\mathbb{E}[Q_2^*] \\ \mathbb{E}[s_{\text{base}}] \end{bmatrix}\right) > \varepsilon
\end{equation}
for some $\varepsilon > 0$ that depends on the data distribution. Furthermore, low-uncertainty examples ($Q_1^* \approx 0, Q_2^* \approx 0$) satisfy $h \to 1$ (convergence to apex), while high-uncertainty examples ($Q_1^* \approx 1, Q_2^* \approx 1$) satisfy $h \to 0$ (remaining near base).
\end{theorem}

\begin{proof}[Proof sketch]
The height consistency loss $\mathcal{L}_{\text{height}}$ penalizes deviations from the derived height. At convergence, $h$ must satisfy the fixed-point equation:
\begin{equation}
h^* = \sigma\left(W_h \cdot \begin{bmatrix} 1-Q_1 \\ 1-Q_2 \\ s_{\text{base}} \end{bmatrix}\right)
\end{equation}
Since $Q_1, Q_2 \in [0,1]$ and $s_{\text{base}} \in [0,1]$, and $W_h$ is learned with positive initialization bias, $h^* > 0$ generically. The vertical gradient pulls low-uncertainty states toward the apex ($h \to 1$) and keeps high-uncertainty states near the base ($h \to 0$), creating a stable stratification. Full proof requires showing Lipschitz continuity of $h(Q_1, Q_2, s_{\text{base}})$ and is deferred to supplementary material.
\end{proof}

\begin{theorem}[Collapse Prevention via Orthogonal Supervision]
\label{thm:collapse_prevention}
Consider the pyramidal VARO loss with independent targets $Q_1^* = 1 - p(y^*)$ and $Q_2^* = \frac{1}{2}[(1-\mathbb{1}_{\text{correct}}) + H(p)/\log V]$. Let $\rho_{Q_1, Q_2} = \text{corr}(Q_1^*, Q_2^*)$ denote the correlation between targets. If $|\rho_{Q_1, Q_2}| < 1$ (targets are not perfectly correlated), then gradient descent with $\lambda_{Q_1}, \lambda_{Q_2} > 0$ prevents simultaneous collapse of both gates. Specifically, at least one of $Q_1$ or $Q_2$ maintains entropy $H(Q_i) > \delta$ for some $\delta > 0.1$ throughout training.
\end{theorem}

\begin{proof}[Proof sketch]
Suppose both gates collapse to constant values $Q_1 \approx c_1$ and $Q_2 \approx c_2$. Then:
\begin{align}
\mathcal{L}_{Q_1} &= \|c_1 - Q_1^*\|^2 = \text{Var}(Q_1^*) + (\mathbb{E}[Q_1^*] - c_1)^2 \\
\mathcal{L}_{Q_2} &= \|c_2 - Q_2^*\|^2 = \text{Var}(Q_2^*) + (\mathbb{E}[Q_2^*] - c_2)^2
\end{align}
Since $Q_1^*$ and $Q_2^*$ have non-zero variance (by assumption $|\rho| < 1$), constant gates incur non-zero loss. Gradients $\nabla_{Q_1} \mathcal{L}$ and $\nabla_{Q_2} \mathcal{L}$ remain non-zero, preventing collapse. In contrast, the tetrahedral formulation used $u = 1 - Q_1 Q_2$ with a single supervision signal, allowing both gates to drift to high values ($Q_1, Q_2 \to 1$) while maintaining $u$ near a target.
\end{proof}

\begin{theorem}[Fractal Stability Bound]
\label{thm:fractal_stability}
Under the fractal regularization loss $\mathcal{L}_{\text{fractal}} = \sigma_{Q_1}^2 + \sigma_{Q_2}^2$ with $\lambda_{\text{fractal}} > 0$, the fractal variances satisfy:
\begin{equation}
\mathbb{E}[\sigma_{Q_i}^2] \leq \frac{C}{\lambda_{\text{fractal}}}
\end{equation}
for some constant $C$ depending on data variance. Furthermore, total uncertainty $U_{\text{total}} = Q_1 + Q_2 \cdot (1 + u_{\text{fractal}})$ is bounded by:
\begin{equation}
U_{\text{total}} \leq Q_1 + Q_2 \cdot \left(1 + \sigma\left(\sqrt{\frac{2C}{\lambda_{\text{fractal}}}}\right)\right)
\end{equation}
preventing fractal uncertainty from exploding.
\end{theorem}

\begin{proof}[Proof sketch]
The $L^2$ regularization on $\sigma_{Q_i}^2$ creates a quadratic penalty. At equilibrium, the gradient from the fractal loss must balance the gradient from data fit. Standard regularization theory~\cite{bishop2006pattern} yields the $1/\lambda$ bound. The total uncertainty bound follows from substituting $u_{\text{fractal}} = \sigma(W_f \cdot [\sigma_{Q_1}, \sigma_{Q_2}])$ and applying Cauchy-Schwarz. Full analysis requires spectral properties of the Hessian and is omitted.
\end{proof}

These three theorems establish that the pyramidal architecture:
\begin{enumerate}[leftmargin=*]
    \item Creates a natural attractor (apex) that prevents horizontal drift (Theorem~\ref{thm:apex_attractor})
    \item Prevents gate collapse via orthogonal supervision of $Q_1$ and $Q_2$ (Theorem~\ref{thm:collapse_prevention})
    \item Bounds fractal meta-uncertainty to prevent runaway inflation (Theorem~\ref{thm:fractal_stability})
\end{enumerate}

\section{Experimental Design}
\subsection{Datasets and Metrics}
\begin{table}[h]
    \centering
    \begin{tabular}{llll}
        \toprule
        Dataset & Task & Metric & Baseline Expected \\
        \midrule
        TruthfulQA~\cite{lin2021truthfulqa} & Hallucination & \% truthful answers & $40\%$ \\
        TempQuestions~\cite{tu2023tempquestions} & Temporal generalization & Accuracy & $30\%$ \\
        Consistency~\cite{elazar2021consistency} & Paraphrase consistency & Accuracy variance & $15\%$ \\
        MMLU~\cite{hendrycks2020mmlu} & Calibration & ECE, Brier score & $0.15$ ECE \\
        Synthetic OOD~\cite{ovadia2019can} & Uncertainty detection & AUROC (unc vs. error) & $0.60$ \\
        \bottomrule
    \end{tabular}
    \caption{Datasets and metrics for evaluating Aletheion.}
    \label{tab:datasets}
\end{table}

\subsection{Models and Ablations}
\begin{table}[h]
\centering
\caption{Projected performance improvements across Aletheion levels.$^{\dagger}$}
\label{tab:results}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{TruthfulQA} & \textbf{ECE} & \textbf{Hallucination Rate} & \textbf{Unc--Error Corr.} \\
\midrule
Baseline Transformer & 40\% & 0.15 & 60\% & 0.30 \\
+ Temperature Scaling & 42\% & 0.13 & 58\% & 0.35 \\
Aletheion Level 1 & 48\% & 0.10 & 45\% & 0.60 \\
Aletheion Level 2 & 52\% & 0.08 & 38\% & 0.70 \\
Aletheion Level 3 & 58\% & 0.06 & 25\% & 0.80 \\
\bottomrule
\end{tabular}

\vspace{0.5em}
\footnotesize{$^{\dagger}$These are theoretical projections based on architectural analysis and prior uncertainty quantification literature. Empirical validation is ongoing and results may vary. The baseline transformer achieves stated performance on respective benchmarks \cite{lin2021truthfulqa}. Projected improvements assume successful VARO training and optimal $\lambda$ tuning.}
\end{table}

Ablations include removing $Q_2$, varying $\lambda$, testing alternative uncertainty aggregators, and evaluating abstention policies. Additional diagnostics compute selective prediction curves, coverage-controlled risk, and retrieval triggers under uncertainty.

\subsection{Evaluation Protocol}
\begin{enumerate}[leftmargin=*]
    \item Pretrain baseline model on open-source corpora.
    \item Fine-tune Levels 1--3 using identical data, enabling incremental comparisons.
    \item Measure calibration via ECE, Brier score, and reliability diagrams.
    \item Report computational overhead (FLOPs, latency) for inference.
    \item Evaluate abstention quality using selective prediction curves and coverage risk.
\end{enumerate}

\subsection{Risk and Mitigation}
Potential failure includes gate collapse and miscalibrated $u^*$. We monitor entropy of gate outputs, apply adaptive $\lambda$, and integrate human-in-the-loop review for high uncertainty outputs.

\input{skynet_phenomenon}

\section{Discussion}
\subsection{Why Fractal Works}
Self-similarity enforces consistent epistemic reasoning across all scales of the transformer. Local attention gates prevent uncertainty collapse at early layers, while global output gates maintain calibrated predictions. The hierarchy mirrors residual networks and multi-scale reasoning observed in compositional attention structures.

Unlike fixed-architecture approaches, Q1Q2 exhibits adaptive epistemic dynamics, discovering optimal uncertainty decomposition for each dataset. This flexibility suggests the architecture could generalize across domains with varying aleatoric/epistemic structure.

\subsection{Limitations and Open Questions}

We categorize open questions by urgency and expected outcomes:

\subsubsection*{Critical (blocking production deployment)}

\paragraph{Q1: Gate collapse}
\textbf{Question:} Can $Q_1$ or $Q_2$ degenerate to always-on ($\approx 1$) or always-off ($\approx 0$)?

\textbf{Expected answer:} Unlikely with entropy regularization.

\textbf{Evidence:} Similar gating mechanisms (e.g., LSTM gates \cite{hochreiter1997long}, attention gates in transformers) avoid collapse when trained with proper regularization.

\textbf{Validation strategy:}
\begin{itemize}
    \item Monitor gate entropy during training: $H(Q_i) = -\sum_j q_{ij} \log q_{ij}$
    \item Apply gradient penalties if $\mathbb{E}[H(Q_i)] < \theta_{\text{min}}$
    \item Use initialization bias: initialize $Q_i$ to output $\approx 0.7$ (confident but not saturated)
\end{itemize}

\paragraph{Q2: VARO-RLHF interaction}
\textbf{Question:} Does preference optimization (DPO/RLHF) after VARO training collapse epistemic gates?

\textbf{Hypothesis:} Sequential training (VARO $\to$ freeze gates $\to$ RLHF) preserves calibration.

\textbf{Alternative approach:} Joint training with multi-objective loss:
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{RLHF}} + \lambda_1 \mathcal{L}_{\text{VARO}} + \lambda_2 H(\text{gates})
\end{equation}

\textbf{Requires:} Empirical validation on standard RLHF benchmarks (HH-RLHF, Anthropic Helpful-Harmless).

\subsubsection*{High priority (affects performance)}

\paragraph{Q3: Optimal $\lambda$}
\textbf{Question:} How to set VARO weight $\lambda$ across datasets and model scales?

\textbf{Current approach:} Grid search $\lambda \in \{0.01, 0.1, 1.0\}$

\textbf{Expected:} $\lambda$ scales inversely with model size (larger models need smaller $\lambda$ to avoid overwhelming cross-entropy signal).

\textbf{Future:} Meta-learning $\lambda$ or adaptive $\lambda_t$ schedule (e.g., cosine annealing).

\paragraph{Q4: Uncertainty aggregation}
\textbf{Question:} Which function $f$ ($\max$/$\text{mean}$/learned) works best?

\textbf{Hypothesis:}
\begin{itemize}
    \item $\max$: best for safety-critical applications (conservative, never under-reports uncertainty)
    \item $\text{mean}$: best for balanced performance-calibration tradeoff
    \item Learned: best asymptotic performance but requires uncertainty-labeled data
\end{itemize}

\textbf{Ablation study:} Test all three on TruthfulQA, compare ECE and selective prediction curves.

\paragraph{Q5: Scaling to 175B+ parameters}
\textbf{Question:} Do uncertainty gains persist at GPT-3 scale (175B) and beyond?

\textbf{Expected:} Yes, since epistemic failures worsen with scale \cite{lin2021truthfulqa}. Larger models hallucinate more complex fabrications.

\textbf{Challenge:} Computational cost of training gates on 175B model ($\approx 5\%$ overhead still substantial).

\textbf{Mitigation strategy:}
\begin{enumerate}
    \item Train smaller epistemic model (e.g., 7B with gates)
    \item Distill uncertainty behavior to large base model (175B)
    \item Use LoRA-style efficient fine-tuning for gates only
\end{enumerate}

\subsubsection*{Medium priority (future work)}

\paragraph{Q6: Multimodal extension}
\textbf{Question:} How to apply epistemic softmax to vision-language models?

\textbf{Approach:} Gated cross-attention between vision and text modalities.

\textbf{Application:} Reduce hallucination in image captioning (e.g., detecting when visual features insufficient to support textual claim).

\paragraph{Q7: Epistemic chain-of-thought}
\textbf{Question:} Can the model reason explicitly about its own uncertainty?

\textbf{Example:} "I'm uncertain about X because evidence Y conflicts with evidence Z."

\textbf{Requires:} Training on uncertainty-annotated reasoning traces (expensive to collect).

\paragraph{Q8: Adversarial robustness}
\textbf{Question:} Can adversarial inputs fool epistemic gates?

\textbf{Risk:} Adversary crafts input that looks out-of-distribution but gates output $u \approx 0$ (false confidence).

\textbf{Defense:} Adversarial training specifically on gates:
\begin{equation}
\max_{\|\delta\| < \epsilon} \mathcal{L}_{\text{VARO}}(x + \delta)
\end{equation}

\paragraph{Q9: Integration with Consistency Training}
\textbf{Question:} How does Aletheion interact with consistency training~\cite{google2024consistency}?

\textbf{Hypothesis:} Complementary and synergistic. Aletheion provides architectural epistemic gates while consistency training enforces paraphrase invariance.

\textbf{Approach:} Combined loss function:
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{CE}} + \lambda_1 \mathcal{L}_{\text{VARO}} + \lambda_2 \mathcal{L}_{\text{consistency}}
\end{equation}
where $\mathcal{L}_{\text{consistency}}$ penalizes output variance across paraphrased prompts.

\textbf{Expected outcome:} Models that are both \emph{calibrated} (low ECE via VARO) and \emph{robust to paraphrases} (low variance via consistency training). This combination may be particularly effective against sycophancy (Failure Mode 3, Section~3).

\textbf{Experimental validation:}
\begin{itemize}
    \item Baseline: Standard transformer
    \item + Consistency training only
    \item + Aletheion only
    \item + Both (combined)
\end{itemize}
Measure: TruthfulQA accuracy, paraphrase consistency, ECE.

\subsubsection*{Low priority (philosophical)}

\paragraph{Q10:} What is the correct formalization of "epistemic uncertainty" for autoregressive language models?

\paragraph{Q11:} Can epistemic gates enable true "I don't know" responses (or just calibrated low confidence)?

\paragraph{Q12:} Relationship to human metacognition and confidence calibration?

\subsection{Philosophical Implications}
Softmax acts as a forced decision rule; epistemic softmax enables ``aware'' decisions where the model can admit ignorance. This architectural humility aligns with AI safety principles emphasizing deferment when knowledge is insufficient~\cite{ji2023survey}.

\subsection{Connection to ARC-AGI}
The Abstraction and Reasoning Corpus (ARC) tests few-shot abstract reasoning where current LLMs underperform (approximately $5\%$ vs. $85\%$ human accuracy)~\cite{chollet2019measure}. Epistemic gating addresses ARC's challenges: (1) ambiguity detection via $Q_2$ detecting conflicting hypotheses, (2) abstention through uncertainty-driven refusal, and (3) hierarchical reasoning by mirroring ARC's multi-level abstractions. We hypothesize Level 3 Aletheion reduces catastrophic failures on ARC-style tasks by refusing uncertain answers and requesting clarification.

\subsection{When Aletheion Fails: Failure Mode Analysis}

Epistemic softmax is not a panacea. We identify scenarios where the architecture cannot provide guarantees:

\subsubsection*{1. Irreducible Aleatoric Uncertainty}

\textbf{Problem:} Inherently random processes (dice rolls, quantum events, inherently unpredictable future events).

\textbf{Why Aletheion fails:} No amount of information reduces uncertainty. Epistemic gates cannot distinguish aleatoric from epistemic uncertainty without explicit supervision.

\textbf{Example:} "Will this fair coin land heads?"
\begin{itemize}
    \item True answer: $p(\text{heads}) = 0.5$ with $u = 1$ (maximal uncertainty, but aleatoric)
    \item Aletheion: May output $p \approx 0.5$ but $u$ may be miscalibrated
\end{itemize}

\textbf{Mitigation:} Distinguish epistemic vs. aleatoric in $u^*$ supervision signal. For known aleatoric scenarios, supervise with $u^* = 1$ but flag as non-reducible.

\subsubsection*{2. Adversarial Attacks on Gates}
\label{sec:adv-attacks}

\textbf{Problem:} Adversary crafts inputs that fool $Q_1$/$Q_2$ into outputting low uncertainty despite the input being adversarial.

\textbf{Example:} Input $x_{\text{adv}}$ that appears in-distribution to gates but is actually crafted to trigger specific (wrong) behavior.

\textbf{Why Aletheion fails:} Gates are neural networks, thus vulnerable to adversarial examples. Standard adversarial training does not explicitly protect gates.

\textbf{Mitigation:} Adversarial training specifically targeting gates:
\begin{equation}
\mathcal{L}_{\text{adv}} = \max_{\|\delta\| < \epsilon} \mathcal{L}_{\text{VARO}}(x + \delta)
\label{eq:adv-varo}
\end{equation}
subject to $\|\delta\|_{\infty} < \epsilon$ (small perturbation).

\textbf{Empirical Demonstration:} During the adversarial gate-training phase (Sec.~\ref{sec:adv-attacks}), the model
reaches \textbf{Height $=1.000$} with nearly silent uncertainty gates
(\textbf{$Q_1\!\approx\!0.03$, $Q_2\!\approx\!0.06$}), consistent with a successful
adversarial suppression of epistemic responses.
This illustrates the theoretical vulnerability described in Eq.~\eqref{eq:adv-varo}:
adversarial optimization of $\mathcal L_{\text{VARO}}$ can induce
\textbf{apex delusion}---overconfident predictions despite residual calibration (ECE $\approx 0.06$).

\subsubsection*{3. Specification Gaming}

\textbf{Problem:} RLHF may incentivize \emph{hiding} uncertainty to maximize reward.

\textbf{Example:} If reward model favors confident answers, the model learns "confident wrong answer gets higher reward than uncertain right answer."

\textbf{Why Aletheion fails:} Preference optimization doesn't value calibration by default. Gates may learn to always output low $u$ to please the reward model.

\textbf{Mitigation:} Include calibration metrics in reward model:
\begin{equation}
R(\text{response}) = R_{\text{preference}}(\text{response}) - \lambda \cdot \text{ECE}(\text{response})
\end{equation}
Penalize miscalibrated confidence directly in the reward.

\subsubsection*{4. Catastrophic Forgetting During Fine-Tuning}

\textbf{Problem:} Fine-tuning on narrow distribution may collapse gates. As illustrated in Step~36{,}300, the model undergoes a transient phase of
\textbf{representational compression} (Memory~$\downarrow$, Pain~$\uparrow$),
consistent with the ``Catastrophic Forgetting'' dynamics described in
Section~\ref{sec:catastrophic-forgetting}.


\textbf{Example:} Fine-tune on medical QA dataset $\to$ gates learn to always be confident on medical queries, but forget to trigger uncertainty on non-medical queries.

\textbf{Why Aletheion fails:} Standard fine-tuning doesn't preserve epistemic behavior outside the fine-tuning distribution.

\textbf{Mitigation:}
\begin{enumerate}
    \item Continual learning techniques: Elastic Weight Consolidation (EWC), PackNet
    \item Maintain separate uncertainty validation set (held-out diverse queries)
    \item Regularize gates during fine-tuning:
    \begin{equation}
    \mathcal{L}_{\text{finetune}} = \mathcal{L}_{\text{task}} + \lambda \|Q_{\text{new}} - Q_{\text{old}}\|^2
    \end{equation}
\end{enumerate}

\subsubsection*{5. Computational Budget Constraints}

\textbf{Problem:} Production systems may not afford 4-5\% overhead.

\textbf{Example:} Real-time chatbot with strict latency requirements (e.g., <50ms response time).

\textbf{Why Aletheion fails:} Even small overhead may violate SLA (service level agreement).

\textbf{Mitigation:}
\begin{enumerate}
    \item Use Level 1 (output-only, <1\% overhead)
    \item Conditional gating: Only activate gates for queries flagged as potentially uncertain (via cheap heuristic)
    \item Model distillation: Train small "epistemic triage" model; use full Aletheion only when triage indicates uncertainty
\end{enumerate}

\subsubsection*{6. Missing Ground Truth for $u^*$}

\textbf{Problem:} Many domains lack uncertainty labels.

\textbf{Example:} Creative writing tasks have no "correct" answer, thus $u^*$ is undefined.

\textbf{Why Aletheion fails:} VARO requires $u^*$ supervision. Without it, gates may not learn meaningful uncertainty.

\textbf{Mitigation:}
\begin{enumerate}
    \item Use unsupervised methods (head variance, self-consistency) as fallback
    \item Human annotation for calibration set (expensive but one-time cost)
    \item Transfer uncertainty behavior from related domains (e.g., factual QA $\to$ creative writing)
\end{enumerate}

\subsubsection*{Summary of Failure Modes}

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Failure Mode} & \textbf{Severity} & \textbf{Mitigation?} & \textbf{Blocks Deploy?} \\
\midrule
Aleatoric uncertainty & Low & Partial (better $u^*$) & No \\
Adversarial gates & Medium & Yes (adv. training) & No \\
Specification gaming & High & Yes (calibration rewards) & Maybe \\
Catastrophic forgetting & High & Yes (continual learning) & No \\
Compute constraints & Medium & Yes (Level 1, distill) & Maybe \\
Missing $u^*$ labels & Medium & Yes (unsupervised) & No \\
\bottomrule
\end{tabular}
\caption{Failure scenarios and recommended mitigations for Aletheion.}
\label{tab:failures}
\end{table}

\textbf{Deployment recommendation:} Deploy Aletheion with continuous monitoring of gate behavior and maintain a held-out uncertainty validation set to detect failures early. Start with Level 1 in production; upgrade to Level 2/3 as compute budget allows.

\section{Related Work}

\subsection{Overconfidence in Neural Networks}

The tendency of neural networks to exhibit overconfidence has been documented extensively~\cite{guo2017calibration,ovadia2019can,lin2021truthfulqa}. This ``Skynet problem'' emerges from three fundamental issues:
\begin{itemize}[leftmargin=*]
    \item \textbf{Softmax saturation:} Driving outputs toward corners of the probability simplex, eliminating nuanced uncertainty
    \item \textbf{Lack of intrinsic uncertainty representation:} No architectural mechanism to express ``I do not know''
    \item \textbf{Optimization pressure:} Cross-entropy loss favoring confident (but wrong) predictions over calibrated uncertainty
\end{itemize}

Our pyramidal architecture addresses these issues through geometric constraints rather than post-hoc corrections. By embedding epistemic gates directly in the architecture, we prevent overconfidence at its source rather than attempting to correct it after training.

\subsection{General Context}

Aletheion builds on transformer advancements~\cite{vaswani2017attention,brown2020language}, scaling studies in language models, hallucination analyses~\cite{ji2023survey,lin2021truthfulqa}, and uncertainty estimation techniques including Bayesian approximations and deep ensembles~\cite{blundell2015weight,gal2016dropout,lakshminarayanan2017simple}. Recent work on eliciting model uncertainty underscores the need for architectural primitives rather than post-hoc estimates~\cite{lin2022teaching,kadavath2022language,malinin2021uncertainty}.

\section{Conclusion}
We introduced Aletheion, a fractal epistemic architecture that replaces all softmax operations with uncertainty-aware epistemic softmax. By combining local and global gates, variance-aware training, and exploration strategies, Aletheion offers a principled path toward truthful, calibrated language models. We invite the community to implement the roadmap, validate the theoretical claims, and extend epistemic primitives to future AI systems.

The Skynet problem is not inevitable. Through geometric constraints---pyramidal height coordinates, simplex-based uncertainty decomposition, and explicit epistemic gates---we can build AI systems that remain calibrated even as they scale. The solution lies not in limiting capability, but in encoding humility architecturally. This is how we solve Skynet: not by preventing AI from becoming powerful, but by ensuring it knows its limits.

\subsection*{Code Availability and Reproducibility}
All code, data, and experimental configurations are publicly available at \url{https://github.com/AletheionAGI/aletheion-llm}. The repository includes comprehensive documentation for installation, training, evaluation, and analysis. We encourage the community to reproduce our results, validate our claims, and extend the Aletheion framework to new domains and architectures.

\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}
