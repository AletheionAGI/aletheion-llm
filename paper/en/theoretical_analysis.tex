\section{Theoretical Analysis}
\subsection{Monotone Uncertainty Propagation}
\begin{theorem}[Uncertainty Propagation]
Let $h^{(l+1)} = f_l(h^{(l)}, p_{\mathrm{gated}}^{(l)})$ denote the representation update at layer $l$ and $u^{(l)}$ the uncertainty emitted by that layer. Suppose aggregation uses a monotone non-decreasing function $f$. Then the final uncertainty satisfies
\begin{equation}
    u_{\mathrm{final}} \geq \max_{0 \leq l \leq L} u^{(l)}.
\end{equation}
\end{theorem}
\begin{proof}[Proof of Theorem 1]
We prove that $u_{\text{final}} \geq \max_{0 \leq l \leq L} u^{(l)}$ for monotone aggregation function $f$.

\textbf{Step 1:} By definition of the aggregation function:
\begin{equation}
u_{\text{final}} = f(u^{(0)}, u^{(1)}, \ldots, u^{(L)})
\end{equation}

\textbf{Step 2:} Let $u_{\max} = \max_{0 \leq l \leq L} u^{(l)}$ and let $l^* \in \{0, \ldots, L\}$ be the layer achieving this maximum:
\begin{equation}
u^{(l^*)} = u_{\max}
\end{equation}

\textbf{Step 3:} Consider the input vector to $f$ where we set all entries except $l^*$ to zero:
\begin{equation}
v_{\min} = (0, 0, \ldots, \underbrace{u_{\max}}_{l^*}, \ldots, 0) \in [0,1]^{L+1}
\end{equation}

\textbf{Step 4:} Since $f$ is monotone non-decreasing in each argument and $u^{(l)} \geq 0$ for all $l$:
\begin{equation}
f(u^{(0)}, \ldots, u^{(L)}) \geq f(v_{\min}) = f(0, \ldots, u_{\max}, \ldots, 0)
\end{equation}

\textbf{Step 5:} For specific aggregation functions:
\begin{itemize}
    \item \textbf{Max aggregator:} $f = \max$
    \begin{equation}
    f(0, \ldots, u_{\max}, \ldots, 0) = u_{\max}
    \end{equation}

    \item \textbf{Mean aggregator:} $f = \text{mean}$
    \begin{equation}
    f(u^{(0)}, \ldots, u^{(L)}) = \frac{1}{L+1} \sum_{l=0}^{L} u^{(l)} \geq \frac{u_{\max}}{L+1}
    \end{equation}
    However, this is a weaker bound. To achieve $u_{\text{final}} \geq u_{\max}$, we require $f$ to satisfy:
    \begin{equation}
    f(\ldots, u_{\max}, \ldots) \geq u_{\max}
    \end{equation}
    which holds for $f = \max$ and learned monotone aggregators with appropriate initialization.

    \item \textbf{Learned aggregator:} Train $f$ to satisfy $f(v) \geq \max_i v_i$ via architectural constraints (e.g., max-pooling layer followed by learned transformation).
\end{itemize}

\textbf{Step 6:} Therefore, for conservative aggregation ($f = \max$):
\begin{equation}
u_{\text{final}} = \max(u^{(0)}, \ldots, u^{(L)}) = \max_{0 \leq l \leq L} u^{(l)}
\end{equation}

\textbf{Step 7:} Residual connections preserve this property because epistemic gates multiply probability distributions rather than subtract scalars. If layer $l$ has high uncertainty $u^{(l)} \approx 1$, the gated distribution $p^{(l)}_{\text{gated}} \approx \text{uniform}$. Subsequent layers cannot "undo" this uncertainty without evidence.

\textbf{Step 8:} Thus, $u_{\text{final}} \geq \max_{l} u^{(l)}$ as required.
\end{proof}

\begin{corollary}
If any layer emits high uncertainty ($u^{(l)}$ close to 1), the final output uncertainty cannot collapse to zero unless all subsequent layers emit perfect certainty ($u = 0$), which is unlikely under VARO training that penalizes miscalibrated confidence.
\end{corollary}

\subsection{Calibration Under VARO}
\begin{theorem}[Calibration Under VARO]
Consider training an epistemic softmax model with stochastic gradient descent on the loss:
\begin{equation}
\mathcal{L}(\theta) = \mathcal{L}_{\text{CE}}(p_{\text{gated}}(\theta), y) + \lambda \|u(\theta) - u^*\|_2^2
\end{equation}
where $\theta$ are model parameters, $p_{\text{gated}}$ is the gated distribution from Algorithm 1, $u$ is predicted uncertainty, and $u^*$ is target uncertainty.

\textbf{Assumptions:}
\begin{enumerate}[label=(A\arabic*)]
    \item \textbf{$L$-smoothness:} The loss $\mathcal{L}$ is $L$-smooth, i.e., for all $\theta, \theta'$:
    \begin{equation}
    \|\nabla \mathcal{L}(\theta) - \nabla \mathcal{L}(\theta')\| \leq L \|\theta - \theta'\|
    \end{equation}

    \item \textbf{Unbiased uncertainty targets:} The target uncertainty $u^*$ satisfies:
    \begin{equation}
    \mathbb{E}[u^* \mid x] = u_{\text{true}}(x)
    \end{equation}
    where $u_{\text{true}}(x)$ is the true epistemic uncertainty for input $x$.

    \item \textbf{Bounded gradient variance:} For stochastic gradients $g_t$:
    \begin{equation}
    \mathbb{E}[\|g_t - \nabla \mathcal{L}(\theta_t)\|^2] \leq \sigma^2
    \end{equation}

    \item \textbf{Robbins-Monro learning rate:} The learning rate $\eta_t$ satisfies:
    \begin{equation}
    \sum_{t=1}^{\infty} \eta_t = \infty \quad \text{and} \quad \sum_{t=1}^{\infty} \eta_t^2 < \infty
    \end{equation}
\end{enumerate}

Under assumptions (A1)-(A4), the expected calibration error (ECE) decreases:
\begin{equation}
\mathbb{E}[\text{ECE}_{t+1}] \leq \mathbb{E}[\text{ECE}_t] - \eta_t \lambda c_1 + \eta_t^2 c_2
\end{equation}
where:
\begin{align}
c_1 &= 2 \mathbb{E}[\|\nabla_u \text{ECE}\|^2] \quad \text{(gradient of ECE w.r.t. uncertainty)} \\
c_2 &= L\sigma^2 + \frac{L^2}{2} \quad \text{(smoothness and variance terms)}
\end{align}

\textbf{Corollary:} Choosing $\eta_t = 1/t$ yields:
\begin{equation}
\mathbb{E}[\text{ECE}_T] \leq \mathbb{E}[\text{ECE}_0] - \lambda c_1 \log(T) + O(1)
\end{equation}
Thus ECE decreases logarithmically with training steps $T$.
\end{theorem}

\begin{proof}[Proof sketch]
\textbf{Step 1:} By $L$-smoothness (A1) and the SGD update rule $\theta_{t+1} = \theta_t - \eta_t g_t$:
\begin{equation}
\mathbb{E}[\mathcal{L}(\theta_{t+1})] \leq \mathbb{E}[\mathcal{L}(\theta_t)] - \eta_t \mathbb{E}[\|\nabla \mathcal{L}(\theta_t)\|^2] + \frac{L\eta_t^2}{2} \mathbb{E}[\|g_t\|^2]
\end{equation}

\textbf{Step 2:} The VARO term $\lambda \|u - u^*\|^2$ provides gradient:
\begin{equation}
\nabla_{\theta} \left(\lambda \|u - u^*\|^2\right) = 2\lambda (u - u^*) \nabla_{\theta} u
\end{equation}

\textbf{Step 3:} By assumption (A2), $\mathbb{E}[u - u^*]$ measures calibration error, which correlates with ECE. Specifically, ECE is defined as:
\begin{equation}
\text{ECE} = \mathbb{E}_{B \in \text{bins}} \left| \frac{1}{|B|} \sum_{i \in B} \mathbb{1}[y_i = \hat{y}_i] - \bar{c}_B \right|
\end{equation}
where $\bar{c}_B$ is the average confidence in bin $B$.

The VARO loss directly optimizes $\|u - u^*\|^2$, and under proper calibration ($u^* = 1 - \text{confidence}$), minimizing this term reduces the gap between confidence and accuracy, thereby reducing ECE.

\textbf{Step 4:} Substituting (A3) and applying standard SGD convergence analysis (see \cite{bottou2018optimization}) with (A4) yields the stated bound.

Full proof requires technical analysis of ECE geometry \cite{guo2017calibration} and is deferred to supplementary material.
\end{proof}

\subsection{Computational Complexity}

\subsubsection*{Standard Transformer Cost}
For $L$ layers, sequence length $n$, and hidden dimension $d$:
\begin{align}
\text{Attention:} \quad & O(L \cdot n^2 \cdot d) \quad \text{(all layers)} \\
\text{Feed-forward:} \quad & O(L \cdot n \cdot d^2) \quad \text{(all layers)} \\
\text{Total:} \quad & O(L \cdot n^2 \cdot d + L \cdot n \cdot d^2)
\end{align}

\subsubsection*{Epistemic Softmax Overhead}
Each gate $Q_i$ is an MLP with hidden size $k$:
\begin{align}
\text{Forward:} \quad & O(d \cdot k + k \cdot 1) = O(d \cdot k) \quad \text{per invocation} \\
\text{Memory:} \quad & O(d \cdot k) \quad \text{parameters per gate}
\end{align}

\subsubsection*{Level-by-Level Analysis}

\paragraph{Level 1 (Output-only)}
\begin{itemize}
    \item \textbf{Gates:} 1 $Q_1$ gate + 1 $Q_2$ gate at output layer
    \item \textbf{Operations:} $2 \times O(d \cdot k)$ per token $= O(n \cdot d \cdot k)$
    \item \textbf{Overhead:}
    \begin{equation}
    \frac{O(n \cdot d \cdot k)}{O(L \cdot n^2 \cdot d)} = \frac{k}{L \cdot n}
    \end{equation}
    \item \textbf{Numerical estimate:} For $k = d/4$, $L = 12$, $n = 512$:
    \begin{equation}
    \text{Overhead} \approx \frac{d/4}{12 \times 512} \approx 0.04\% \quad \text{(negligible)}
    \end{equation}
\end{itemize}

\paragraph{Level 2 (Attention + Output)}
\begin{itemize}
    \item \textbf{Gates per layer:}
    \begin{itemize}
        \item $H$ attention heads $\times$ 1 $Q_1$ gate each $= H \times O(d \cdot k)$
        \item 1 $Q_2$ gate for head aggregation $= O(d \cdot k)$
        \item Total per layer: $O(H \cdot d \cdot k)$
    \end{itemize}
    \item \textbf{Gates across $L$ layers:} $L \times O(H \cdot n \cdot d \cdot k)$
    \item \textbf{Plus output gates:} $O(n \cdot d \cdot k)$
    \item \textbf{Total overhead:}
    \begin{equation}
    \frac{O(L \cdot H \cdot n \cdot d \cdot k)}{O(L \cdot n^2 \cdot d)} = \frac{H \cdot k}{n}
    \end{equation}
    \item \textbf{Numerical estimate:} For $H = 8$, $k = d/4$, $n = 512$:
    \begin{equation}
    \text{Overhead} = \frac{8 \cdot (d/4)}{512} = \frac{2d}{512} \approx 2\% \quad \text{(for $d = 512$)}
    \end{equation}
    \item \textbf{Range:} 2-3\% depending on $d/n$ ratio
\end{itemize}

\paragraph{Level 3 (Full Fractal)}
\begin{itemize}
    \item \textbf{Additional gates:} MoE routers, adaptive attention mechanisms, etc.
    \item \textbf{Estimate:} Approximately $1.5\times$ Level 2 overhead
    \item \textbf{Total overhead:} $\approx$ 4-5\%
\end{itemize}

\subsubsection*{Parameter Overhead}

\paragraph{Without parameter sharing:}
\begin{itemize}
    \item $Q_1$ gate: $d \times k + k \times 1 \approx d \cdot k$ parameters
    \item $Q_2$ gate: $d \times k$ parameters
    \item \textbf{Level 1:} 2 gates $= 2dk$
    \begin{itemize}
        \item For $k = d/4$: $2d \cdot (d/4) = d^2/2$ parameters
        \item Baseline has $\approx 12d^2$ (for $L=12$ layers $\times$ projection matrices)
        \item Overhead: $(d^2/2)/(12d^2) \approx 4\%$ parameters
    \end{itemize}
    \item \textbf{Level 2:} $2L(H+1)$ gates
    \begin{itemize}
        \item For $L=12$, $H=8$: $2 \cdot 12 \cdot 9 = 216$ gates
        \item Parameters: $216 \cdot dk = 54d^2$ (for $k=d/4$)
        \item Overhead: $54d^2 / (12d^2 \cdot L) \approx 38\%$ parameters (significant!)
    \end{itemize}
\end{itemize}

\paragraph{With parameter sharing (recommended):}
\begin{itemize}
    \item Share $Q_1$ weights across all heads
    \item Share $Q_2$ weights across all layers
    \item \textbf{Level 2 parameters:} Just 2 gates $= 2dk$
    \item \textbf{Overhead:} $< 5\%$ even for Level 3
\end{itemize}

\subsubsection*{Memory-Computation Tradeoff}
\begin{itemize}
    \item \textbf{Without sharing:} Higher memory, same compute per forward pass
    \item \textbf{With sharing:} Lower memory, same compute per forward pass
    \item \textbf{Recommendation:} Share $Q_1$ across heads within a layer; unique $Q_2$ per layer to capture layer-specific consensus patterns
\end{itemize}

\subsubsection*{Empirical Measurement}
To be added after implementation:
\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Latency (ms/token)} & \textbf{Memory (GB)} & \textbf{Overhead} \\
\midrule
Baseline       & $X$ & $Y$ & --- \\
Level 1        & $X + \delta_1$ & $Y$ & $+0.5\%$ \\
Level 2        & $X + \delta_2$ & $Y + \epsilon$ & $+2.5\%$ \\
Level 3        & $X + \delta_3$ & $Y + \epsilon$ & $+4.5\%$ \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Robustness to Gate Collapse}
Gate collapse occurs when $Q_1$ or $Q_2$ saturate at 0 or 1. Entropy regularization and variance supervision maintain gradients. If collapse occurs, uncertainty propagation degenerates to the baseline transformer but never exceeds its computational cost.

\subsection{Pyramidal Stability Theorems}

We now formalize three theorems specific to the pyramidal architecture that establish its superiority over the tetrahedral formulation.

\begin{theorem}[Apex Attractor Property]
\label{thm:apex_attractor}
Let $\mathbf{s}(t) = (1-h(t)) \cdot \mathbf{b}(t) + h(t) \cdot \mathbf{apex}$ be the pyramidal state at training step $t$, where $h(t)$ is derived via Equation~3.2. Under the pyramidal VARO loss (Section~6.2) with $\lambda_{\text{height}} > 0$ and assuming convergence of $Q_1 \to Q_1^*$, $Q_2 \to Q_2^*$, the height coordinate satisfies:
\begin{equation}
\lim_{t \to \infty} \mathbb{E}[h(t)] = \sigma\left(W_h \cdot \begin{bmatrix} 1-\mathbb{E}[Q_1^*] \\ 1-\mathbb{E}[Q_2^*] \\ \mathbb{E}[s_{\text{base}}] \end{bmatrix}\right) > \varepsilon
\end{equation}
for some $\varepsilon > 0$ that depends on the data distribution. Furthermore, low-uncertainty examples ($Q_1^* \approx 0, Q_2^* \approx 0$) satisfy $h \to 1$ (convergence to apex), while high-uncertainty examples ($Q_1^* \approx 1, Q_2^* \approx 1$) satisfy $h \to 0$ (remaining near base).
\end{theorem}

\begin{proof}[Proof sketch]
The height consistency loss $\mathcal{L}_{\text{height}}$ penalizes deviations from the derived height. At convergence, $h$ must satisfy the fixed-point equation:
\begin{equation}
h^* = \sigma\left(W_h \cdot \begin{bmatrix} 1-Q_1 \\ 1-Q_2 \\ s_{\text{base}} \end{bmatrix}\right)
\end{equation}
Since $Q_1, Q_2 \in [0,1]$ and $s_{\text{base}} \in [0,1]$, and $W_h$ is learned with positive initialization bias, $h^* > 0$ generically. The vertical gradient pulls low-uncertainty states toward the apex ($h \to 1$) and keeps high-uncertainty states near the base ($h \to 0$), creating a stable stratification. Full proof requires showing Lipschitz continuity of $h(Q_1, Q_2, s_{\text{base}})$ and is deferred to supplementary material.
\end{proof}

\begin{theorem}[Collapse Prevention via Orthogonal Supervision]
\label{thm:collapse_prevention}
Consider the pyramidal VARO loss with independent targets $Q_1^* = 1 - p(y^*)$ and $Q_2^* = \frac{1}{2}[(1-\mathbb{1}_{\text{correct}}) + H(p)/\log V]$. Let $\rho_{Q_1, Q_2} = \text{corr}(Q_1^*, Q_2^*)$ denote the correlation between targets. If $|\rho_{Q_1, Q_2}| < 1$ (targets are not perfectly correlated), then gradient descent with $\lambda_{Q_1}, \lambda_{Q_2} > 0$ prevents simultaneous collapse of both gates. Specifically, at least one of $Q_1$ or $Q_2$ maintains entropy $H(Q_i) > \delta$ for some $\delta > 0.1$ throughout training.
\end{theorem}

\begin{proof}[Proof sketch]
Suppose both gates collapse to constant values $Q_1 \approx c_1$ and $Q_2 \approx c_2$. Then:
\begin{align}
\mathcal{L}_{Q_1} &= \|c_1 - Q_1^*\|^2 = \text{Var}(Q_1^*) + (\mathbb{E}[Q_1^*] - c_1)^2 \\
\mathcal{L}_{Q_2} &= \|c_2 - Q_2^*\|^2 = \text{Var}(Q_2^*) + (\mathbb{E}[Q_2^*] - c_2)^2
\end{align}
Since $Q_1^*$ and $Q_2^*$ have non-zero variance (by assumption $|\rho| < 1$), constant gates incur non-zero loss. Gradients $\nabla_{Q_1} \mathcal{L}$ and $\nabla_{Q_2} \mathcal{L}$ remain non-zero, preventing collapse. In contrast, the tetrahedral formulation used $u = 1 - Q_1 Q_2$ with a single supervision signal, allowing both gates to drift to high values ($Q_1, Q_2 \to 1$) while maintaining $u$ near a target.
\end{proof}

\begin{theorem}[Fractal Stability Bound]
\label{thm:fractal_stability}
Under the fractal regularization loss $\mathcal{L}_{\text{fractal}} = \sigma_{Q_1}^2 + \sigma_{Q_2}^2$ with $\lambda_{\text{fractal}} > 0$, the fractal variances satisfy:
\begin{equation}
\mathbb{E}[\sigma_{Q_i}^2] \leq \frac{C}{\lambda_{\text{fractal}}}
\end{equation}
for some constant $C$ depending on data variance. Furthermore, total uncertainty $U_{\text{total}} = Q_1 + Q_2 \cdot (1 + u_{\text{fractal}})$ is bounded by:
\begin{equation}
U_{\text{total}} \leq Q_1 + Q_2 \cdot \left(1 + \sigma\left(\sqrt{\frac{2C}{\lambda_{\text{fractal}}}}\right)\right)
\end{equation}
preventing fractal uncertainty from exploding.
\end{theorem}

\begin{proof}[Proof sketch]
The $L^2$ regularization on $\sigma_{Q_i}^2$ creates a quadratic penalty. At equilibrium, the gradient from the fractal loss must balance the gradient from data fit. Standard regularization theory~\cite{bishop2006pattern} yields the $1/\lambda$ bound. The total uncertainty bound follows from substituting $u_{\text{fractal}} = \sigma(W_f \cdot [\sigma_{Q_1}, \sigma_{Q_2}])$ and applying Cauchy-Schwarz. Full analysis requires spectral properties of the Hessian and is omitted.
\end{proof}

These three theorems establish that the pyramidal architecture:
\begin{enumerate}[leftmargin=*]
    \item Creates a natural attractor (apex) that prevents horizontal drift (Theorem~\ref{thm:apex_attractor})
    \item Prevents gate collapse via orthogonal supervision of $Q_1$ and $Q_2$ (Theorem~\ref{thm:collapse_prevention})
    \item Bounds fractal meta-uncertainty to prevent runaway inflation (Theorem~\ref{thm:fractal_stability})
\end{enumerate}

