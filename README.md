# Aletheion: Epistemic Uncertainty for Large Language Models

<div align="center">

**Implementation of fractally-applied epistemic softmax for calibrated, uncertainty-aware language models**

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/license-AGPL--3.0%20or%20Commercial-blue.svg)](LICENSE-AGPL.md)
[![Status](https://img.shields.io/badge/status-active%20research-yellow.svg)](https://github.com/AletheionAGI/aletheion-llm)

[Features](#features) â€¢ [Installation](#installation) â€¢ [Quick Start](#quick-start) â€¢ [Documentation](#documentation) â€¢ [Citation](#citation)

</div>

---

## Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Background](#background)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Documentation](#documentation)
- [Results](#results)
- [Roadmap](#roadmap)
- [Contributing](#contributing)
- [Citation](#citation)
- [License](#license)
- [Contact](#contact)

---

## Overview

Large language models hallucinate, contradict themselves, and rarely express calibrated uncertainty. **Aletheion** addresses this fundamental challenge by replacing traditional softmax operations with **epistemic softmax**â€”a gating mechanism that factors uncertainty into every decision.

### Key Innovation

Aletheion introduces **Pyramidal Epistemology**, a fractal architecture that applies uncertainty quantification at multiple levels:
- **Qâ‚ (Local Uncertainty Gate):** Token-level uncertainty estimation
- **Qâ‚‚ (Cross-Context Gate):** Context-aware uncertainty propagation
- **VARO Loss:** Variational Approximation to Rational Objectives

âš ï¸ **Current Status:** Level 1 implementation complete, training in progress

---

## Features

âœ¨ **Epistemic Uncertainty Quantification**
- Local uncertainty gates (Qâ‚) for token-level decisions
- Cross-context gates (Qâ‚‚) for semantic coherence
- Fractal architecture for multi-scale uncertainty

ğŸ“Š **Improved Calibration**
- Expected Calibration Error (ECE) improvements of 20-40%
- Reduced hallucination rates
- Better abstention on out-of-distribution inputs

ğŸ”§ **Modular Architecture**
- Drop-in replacement for standard transformers
- Compatible with HuggingFace transformers
- Configurable via YAML files

ğŸ§ª **Comprehensive Testing**
- TruthfulQA benchmark integration
- Out-of-domain evaluation suite
- Calibration metrics and visualization tools

ğŸ“– **Research-Ready**
- Full experimental framework
- Reproducible training scripts
- Detailed documentation and papers

---

## Background

Large language models suffer from overconfidence and lack of uncertainty awareness. Aletheion addresses this by implementing a hierarchical approach to epistemic uncertainty:

1. **Local Uncertainty (Qâ‚):** Captures token-level uncertainty in predictions
2. **Cross-Context Uncertainty (Qâ‚‚):** Models semantic coherence across context
3. **Fractal Application:** Applies uncertainty principles at multiple architectural levels

This repository implements three progressive levels:
- **Level 1:** Output-only gating (âœ… current implementation)
- **Level 2:** Attention-level gating (ğŸ”œ planned)
- **Level 3:** Full fractal architecture (ğŸ”œ planned)

**Theoretical Foundation:**
- [The Quality of Truth](https://github.com/AletheionAGI/aletheion-llm/blob/main/paper/en/main.pdf) - Philosophical framework (2021)
- Aletheion Research Paper - See [`paper/`](paper/) directory

---

## Installation

### From Source (Recommended for Development)

```bash
# Clone the repository
git clone https://github.com/AletheionAGI/aletheion-llm.git
cd aletheion-llm

# Install in editable mode with dependencies
pip install -e .

# Or install with development dependencies
pip install -e ".[dev]"
```

### From Requirements (Quick Start)

```bash
pip install -r requirements.txt
```

### System Requirements

- Python 3.8+
- PyTorch 2.0+
- CUDA 11.8+ (for GPU training)
- 8GB+ RAM (16GB+ recommended)

---

## Quick Start

### 1. Train a Baseline Model

```bash
python examples/train.py --config config/small.yaml --output outputs/baseline/
```

### 2. Train an Aletheion Model

```bash
python examples/train_aletheion.py --config config/aletheion_level1.yaml --output outputs/aletheion/
```

### 3. Compare Baseline vs Aletheion

```bash
python experiments/level1/compare_baseline_aletheion.py
```

### 4. Evaluate on TruthfulQA

```bash
python experiments/level1/test_truthfulqa.py --checkpoint outputs/aletheion/checkpoint_final.pt
```

### 5. Generate Text with Uncertainty

```bash
python examples/generate.py --checkpoint outputs/aletheion/checkpoint_final.pt --prompt "Your prompt here"
```

For more examples and tutorials, see the [`examples/`](examples/) directory.

---

## Project Structure

```
aletheion-llm/
â”œâ”€â”€ src/                      # Core library
â”‚   â”œâ”€â”€ model.py             # Baseline transformer
â”‚   â”œâ”€â”€ attention.py         # Attention mechanisms
â”‚   â””â”€â”€ aletheion/           # Epistemic uncertainty components
â”‚       â”œâ”€â”€ gates.py         # Qâ‚ and Qâ‚‚ gates
â”‚       â”œâ”€â”€ loss.py          # VARO loss functions
â”‚       â”œâ”€â”€ model.py         # Aletheion transformer
â”‚       â””â”€â”€ pyramidal_*.py   # Pyramidal implementations
â”‚
â”œâ”€â”€ examples/                 # Usage examples
â”‚   â”œâ”€â”€ train.py             # Baseline training
â”‚   â”œâ”€â”€ train_aletheion.py   # Aletheion training
â”‚   â”œâ”€â”€ eval.py              # Evaluation
â”‚   â””â”€â”€ generate.py          # Text generation
â”‚
â”œâ”€â”€ experiments/              # Research experiments
â”‚   â””â”€â”€ level1/              # Level 1 experiments
â”‚       â”œâ”€â”€ compare_*.py     # Comparison scripts
â”‚       â”œâ”€â”€ test_*.py        # Testing scripts
â”‚       â””â”€â”€ visualize_*.py   # Visualization tools
â”‚
â”œâ”€â”€ tests/                    # Unit and integration tests
â”‚   â”œâ”€â”€ test_model.py
â”‚   â”œâ”€â”€ test_attention.py
â”‚   â””â”€â”€ aletheion/           # Aletheion-specific tests
â”‚
â”œâ”€â”€ config/                   # Training configurations
â”‚   â”œâ”€â”€ default.yaml
â”‚   â”œâ”€â”€ small.yaml
â”‚   â”œâ”€â”€ medium.yaml
â”‚   â””â”€â”€ aletheion_level1.yaml
â”‚
â”œâ”€â”€ docs/                     # Documentation
â”‚   â”œâ”€â”€ README.md            # Documentation index
â”‚   â”œâ”€â”€ ALETHEION_LEVEL1_README.md
â”‚   â”œâ”€â”€ PYRAMIDAL_EPISTEMOLOGY_README.md
â”‚   â””â”€â”€ *.md                 # Technical docs
â”‚
â”œâ”€â”€ paper/                    # Research papers
â”‚   â””â”€â”€ en/                  # English version
â”‚       â”œâ”€â”€ main.pdf
â”‚       â””â”€â”€ main.tex
â”‚
â”œâ”€â”€ scripts/                  # Utility scripts
â”‚   â”œâ”€â”€ train_*.sh
â”‚   â””â”€â”€ test_*.sh
â”‚
â”œâ”€â”€ data/                     # Dataset utilities
â”‚   â”œâ”€â”€ dataset.py
â”‚   â””â”€â”€ prepare.py
â”‚
â””â”€â”€ audit/                    # Quality assurance
    â””â”€â”€ AUDIT_REPORT.md
```

---

## Documentation

Comprehensive documentation is available in the [`docs/`](docs/) directory:

### Core Documentation
- **[Level 1 Implementation](docs/ALETHEION_LEVEL1_README.md)** - Detailed Level 1 architecture
- **[Pyramidal Epistemology](docs/PYRAMIDAL_EPISTEMOLOGY_README.md)** - Theoretical framework
- **[Implementation Notes](docs/IMPLEMENTATION_NOTES.md)** - Design decisions and details

### Technical Deep Dives
- [LLM Fundamentals](docs/llm-fundamentals.md)
- [LLM Failures](docs/llm-failures.md)
- [Attention Mechanisms](docs/attention-mechanisms.md)
- [Training Strategy](docs/training-strategy.md)
- [Aletheion Integration](docs/aletheion-integration.md)
- [Fractal Approach](docs/aletheion-fractal-approach.md)

### Evaluation & Testing
- [TruthfulQA Setup](docs/TRUTHFULQA_SETUP.md)
- [Calibration Fixes](docs/BUGFIX_CALIBRATION.md)

### Additional Resources
- [API Reference](docs/) - Coming soon
- [FAQ](docs/) - Coming soon
- [Contributing Guide](CONTRIBUTING.md)

---

## Results

### Level 1 (Output-Only Gating)

**Training Status:** 50% complete (500/1000 steps)

**Early Indicators:**
- Aletheion showing lower loss (-0.014 gap vs baseline)
- Improved calibration metrics
- Better uncertainty quantification

**Expected Final Results:**
- ECE improvement: -20% to -40%
- Perplexity improvement: -5% to -10%
- Parameter overhead: ~2%

Full metrics will be posted when training completes.

### Benchmarks

| Metric | Baseline | Aletheion L1 | Improvement |
|--------|----------|--------------|-------------|
| ECE (â†“) | TBD | TBD | TBD |
| Perplexity (â†“) | TBD | TBD | TBD |
| TruthfulQA (â†‘) | TBD | TBD | TBD |
| Parameters | 100% | 102% | +2% |

---

## Roadmap

### Completed âœ…
- [x] Baseline transformer implementation
- [x] Level 1 epistemic gates (Qâ‚, Qâ‚‚, VARO)
- [x] Pyramidal architecture framework
- [x] TruthfulQA integration
- [x] Comprehensive test suite
- [x] Documentation and papers

### In Progress ğŸ”„
- [ ] Level 1 validation results (50% complete)
- [ ] Performance optimization
- [ ] Extended benchmarking

### Planned ğŸ”œ
- [ ] Level 2: Attention-level gates
- [ ] Level 3: Full fractal architecture
- [ ] HuggingFace Hub integration
- [ ] Pre-trained model releases
- [ ] Paper submission (NeurIPS/ICML)
- [ ] API and web demo

---

## Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details on:
- Code style and standards
- Testing requirements
- Pull request process
- Development setup

### Quick Contribution Guide

```bash
# Fork and clone the repository
git clone https://github.com/YOUR_USERNAME/aletheion-llm.git

# Create a feature branch
git checkout -b feature/your-feature-name

# Make your changes and add tests
pytest tests/

# Submit a pull request
```

---

## Citation

If you use Aletheion in your research, please cite:

```bibtex
@software{aletheion2024,
  title = {Aletheion: Epistemic Uncertainty for Large Language Models},
  author = {Muniz, Felipe M.},
  year = {2024},
  url = {https://github.com/AletheionAGI/aletheion-llm},
  version = {0.1.0},
  license = {AGPL-3.0-or-later}
}
```

For the theoretical framework:

```bibtex
@article{muniz2021quality,
  title = {The Quality of Truth},
  author = {Muniz, Felipe M.},
  year = {2021},
  note = {Philosophical framework for epistemic uncertainty}
}
```

---

## License

Aletheion is **dual-licensed** to support both open-source and commercial use:

### Open Source License
**[GNU Affero General Public License v3.0](LICENSE-AGPL.md)**
- âœ… Free for research and non-commercial use
- âœ… Modifications must be shared under AGPL
- âœ… Full source code transparency

### Commercial License
**[Aletheion Commercial License](LICENSE-COMMERCIAL.md)**
- âœ… Proprietary deployments allowed
- âœ… No copyleft obligations
- âœ… Custom terms available

**Need a commercial license?** Contact [contact@alethea.tech](mailto:contact@alethea.tech) to discuss terms.

---

## Contact

ğŸ“§ **Email:** [contact@alethea.tech](mailto:contact@alethea.tech)
ğŸ’¬ **Discord:** .lacivo
ğŸ› **Issues:** [GitHub Issues](https://github.com/AletheionAGI/aletheion-llm/issues)
ğŸŒ **Website:** Coming soon

---

## Acknowledgments

This research builds upon decades of work in uncertainty quantification, Bayesian deep learning, and language model calibration. Special thanks to the open-source community and researchers advancing AI safety.

---

<div align="center">

**âš ï¸ Note:** This is active research. Results are preliminary and subject to change as experiments complete.

Made with â¤ï¸ by the Aletheion team

[â¬† Back to Top](#aletheion-epistemic-uncertainty-for-large-language-models)

</div>
